{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e290ade5-ba1b-46db-8d34-fb6270c0ba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate imports.\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from matplotlib import pylab as P\n",
    "import matplotlib as plt1\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# import saliency2.core as saliency\n",
    "import time\n",
    "from skimage import segmentation\n",
    "from skimage.morphology import dilation\n",
    "from skimage.morphology import disk\n",
    "from skimage.transform import resize\n",
    "from itertools import combinations\n",
    "import copy\n",
    "import scipy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2cac9d-58a2-4b53-b01a-0f0a93bf2774",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2cdc0ae-19fe-4ea2-9664-d4220b22ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_model(model_name):\n",
    "#     if model_name=='VGG16':\n",
    "#         m2 = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=True)\n",
    "#         model2 = tf.keras.models.Model([m2.inputs], m2.output)\n",
    "#         return model2\n",
    "#     if model_name=='InceptionV3':\n",
    "#         m2 = tf.keras.applications.inception_v3.InceptionV3(weights='imagenet', include_top=True)\n",
    "#         model2 = tf.keras.models.Model([m2.inputs], m2.output)\n",
    "#         return model2\n",
    "#     if model_name=='ResNet50':\n",
    "#         m2 = tf.keras.applications.resnet50.ResNet50(include_top=True,weights='imagenet')\n",
    "#         model2 = tf.keras.models.Model([m2.inputs], m2.output)\n",
    "#         return model2\n",
    "\n",
    "\n",
    "# # model_to_use='ResNet50'\n",
    "# # model_in_use=load_model(model_to_use)\n",
    "# # model_in_use.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d160112-d40d-4073-827a-f6206093fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_softmax_model(m2):\n",
    "    r=m2.layers[-2].output\n",
    "    # new_m=tf.keras.backend.function([m2.inputs],r)\n",
    "    # new_m=tf.keras.models.Model([m2.inputs],r)\n",
    "    new_m = tf.keras.layers.Dense(units=m2.layers[-1].units)(r)\n",
    "    new_m=tf.keras.models.Model([m2.inputs],new_m)\n",
    "    new_m.layers[-1].set_weights(m2.layers[-1].get_weights())\n",
    "    # new_m.summary()\n",
    "    return new_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8cbe9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    if model_name=='VGG16':\n",
    "        m2 = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=True)\n",
    "        # print(m2.layers[-1].activation)\n",
    "        # m2.summary()\n",
    "        model2 = tf.keras.models.Model([m2.inputs], m2.output)\n",
    "        model3=load_pre_softmax_model(m2)\n",
    "        return model2,model3\n",
    "    if model_name=='InceptionV3':\n",
    "        m2 = tf.keras.applications.inception_v3.InceptionV3(weights='imagenet', include_top=True)\n",
    "        model2 = tf.keras.models.Model([m2.inputs], m2.output)\n",
    "        model3=load_pre_softmax_model(m2)\n",
    "        return model2,model3\n",
    "    if model_name=='ResNet50':\n",
    "        m2 = tf.keras.applications.resnet50.ResNet50(include_top=True,weights='imagenet')\n",
    "        model2 = tf.keras.models.Model([m2.inputs], m2.output)\n",
    "        model3=load_pre_softmax_model(m2)\n",
    "        return model2,model3\n",
    "\n",
    "\n",
    "# model_to_use='ResNet50'\n",
    "# model_in_use=load_model(model_to_use)\n",
    "# model_in_use.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bf39f49-643a-4a56-b647-db983171cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m2 = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=True)\n",
    "# model2 = tf.keras.models.Model([m2.inputs], m2.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "572b321e-802f-462f-93a8-a8a48513ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.Sequential([\n",
    "#     hub.KerasLayer(\n",
    "#         name='inception_v1',\n",
    "#         handle='https://tfhub.dev/google/imagenet/inception_v1/classification/4',\n",
    "#         trainable=False),\n",
    "# ])\n",
    "# model.build([None, 224, 224, 3])\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# def load_imagenet_labels(file_path):\n",
    "#   labels_file = tf.keras.utils.get_file('ImageNetLabels.txt', file_path)\n",
    "#   with open(labels_file) as reader:\n",
    "#     f = reader.read()\n",
    "#     labels = f.splitlines()\n",
    "#   return np.array(labels)\n",
    "# imagenet_labels = load_imagenet_labels('https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
    "\n",
    "\n",
    "# def top_k_predictions(img, k=5):\n",
    "#   image_batch = tf.expand_dims(img, 0)\n",
    "#   predictions = model(image_batch)\n",
    "#   probs = tf.nn.softmax(predictions, axis=-1)\n",
    "#   top_probs, top_idxs = tf.math.top_k(input=probs, k=k)\n",
    "#   top_labels = imagenet_labels[tuple(top_idxs)]\n",
    "#   return top_labels, top_probs[0]\n",
    "\n",
    "\n",
    "# pred_label, pred_prob = top_k_predictions(im)\n",
    "# for label, prob in zip(pred_label, pred_prob):\n",
    "#     print(f'{label}: {prob:0.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9302f794-3f8d-4a1c-a4e4-48289a0d134a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9964e9a-c0bf-4462-8775-8b17203452ec",
   "metadata": {},
   "source": [
    "# segment anything model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb35227-91cd-4555-ba28-227cd3788d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"./segment-anything-main/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "# device = \"mps\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "# sam.to(device=device)\n",
    "\n",
    "# mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "mask_generator_2 = SamAutomaticMaskGenerator(\n",
    "    model=sam,\n",
    "    points_per_side=32,\n",
    "    pred_iou_thresh=0.86,\n",
    "    stability_score_thresh=0.92,\n",
    "    crop_n_layers=1,\n",
    "    crop_n_points_downscale_factor=2,\n",
    "    min_mask_region_area=100,  # Requires open-cv to run post-processing\n",
    ")\n",
    "\n",
    "def seg_any(image):\n",
    "    masks_2 = mask_generator_2.generate(image)\n",
    "    masks_new=[i['segmentation'] for i in masks_2]\n",
    "    return masks_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d685dfad-8a41-40be-a6de-7cb32011719e",
   "metadata": {},
   "source": [
    "# segmenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fec8ae-8cc0-4d3a-b2f4-421f5a82de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_segments(image,pth,redo=False):\n",
    "    if os.path.exists(pth+'_segs'+'.pkl') and redo==False :\n",
    "        with open(pth+'_segs'+'.pkl', 'rb') as file:\n",
    "            segs=pickle.load(file)\n",
    "        print('loaded segments from file')\n",
    "        return segs\n",
    "\n",
    "    else:\n",
    "        print('started segmentation')\n",
    "        start_time = time.time()\n",
    "        segs=seg_any(image)\n",
    "        # segs=_get_segments_felzenszwalb(im,min_size=50) #change here if there is a change in shape\n",
    "        #segs=_get_segments_felzenszwalb(im,scale_list=[10],min_size=10) #change here if there is a change in shape\n",
    "        # segs=saliency.xrai._get_segments_felzenszwalb(im,scale_list=[10],min_size=10) #change here if there is a change in shape\n",
    "        # segs=get_masks()\n",
    "        end_time = time.time()\n",
    "        print('time taken by segmentation : '+str(end_time-start_time)+' sec')\n",
    "        print('completed segmentation')\n",
    "        with open(pth+'_segs'+'.pkl', 'wb') as file:\n",
    "            pickle.dump(segs, file)\n",
    "            file.close()\n",
    "\n",
    "        return segs\n",
    "\n",
    "\n",
    "def make_segs_disjoint(segs):\n",
    "    x=segs[0]*1\n",
    "    for i in range(1,len(segs)):\n",
    "        x=x+(segs[i]*(1.01**i))\n",
    "    new_segs=[]\n",
    "    for k in np.unique(x):\n",
    "        new_segs.append(x==k)\n",
    "    return new_segs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994efbd-3446-48c7-9a16-6ebe731e20fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d86e2eb-47d6-4c8d-9b59-d94e4b794471",
   "metadata": {},
   "source": [
    "# Image Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "549e9117-f47c-479c-b9f7-6a73faa95345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_size(model_to_use):\n",
    "    if model_to_use=='VGG16' or model_to_use=='ResNet50':\n",
    "        return 224,224\n",
    "    if model_to_use=='InceptionV3':\n",
    "        return 299,299\n",
    "    \n",
    "    \n",
    "def image_load(model_to_use,file_path):\n",
    "    print('*******************************************************************')\n",
    "    fig_name=file_path.split('/')[-1]\n",
    "    print(fig_name)\n",
    "    \n",
    "    image = PIL.Image.open(file_path)\n",
    "    # image = image.resize((224,224))\n",
    "    target_width, target_height = input_size(model_to_use)\n",
    "    image = resize_with_pad(image, target_width, target_height, padding_color=(0, 0, 0))\n",
    "    image=np.asarray(image)\n",
    "\n",
    "    # image = tf.io.read_file(file_path)\n",
    "    # image = tf.io.decode_jpeg(image, channels=3)\n",
    "    # image = tf.image.resize_with_pad(image, target_height=224, target_width=224)\n",
    "    # image=np.array(image)\n",
    "    \n",
    "    return (fig_name,image)\n",
    "\n",
    "\n",
    "def resize_with_pad(image, target_width, target_height, padding_color=(0, 0, 0)):\n",
    "    # Calculate the new size preserving the aspect ratio\n",
    "    original_width, original_height = image.size\n",
    "    aspect_ratio = original_width / original_height\n",
    "    \n",
    "    if target_width / target_height > aspect_ratio:\n",
    "        # Width is the constraining dimension\n",
    "        new_height = target_height\n",
    "        new_width = int(aspect_ratio * new_height)\n",
    "    else:\n",
    "        # Height is the constraining dimension\n",
    "        new_width = target_width\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "    \n",
    "    # Resize the image to the new size\n",
    "    resized_image = image.resize((new_width, new_height), PIL.Image.BILINEAR)\n",
    "    \n",
    "    # Create a new image with the target size and the padding color\n",
    "    new_image = PIL.Image.new(\"RGB\", (target_width, target_height), padding_color)\n",
    "    \n",
    "    # Calculate the position to paste the resized image onto the new image\n",
    "    paste_position = (\n",
    "        (target_width - new_width) // 2,\n",
    "        (target_height - new_height) // 2\n",
    "    )\n",
    "    \n",
    "    # Paste the resized image onto the new image\n",
    "    new_image.paste(resized_image, paste_position)\n",
    "    \n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdfdc33-a234-45fb-9795-e1f58f6f9da8",
   "metadata": {},
   "source": [
    "# creating required folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13ca80fd-4fe4-4804-a467-d228cb2b460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_folders(pth):\n",
    "#     if not os.path.exists(pth):\n",
    "#         os.system('mkdir '+pth)\n",
    "#     all_folders=['xrai_attributions/','xrai_predarrays/','xrai_intermediate_images/','xrai_graphs/']\n",
    "#     for f in all_folders:\n",
    "#         if not os.path.exists(pth+f):\n",
    "#             os.system('mkdir '+pth+f)\n",
    "#     for f in all_folders[1:]:\n",
    "#         if not os.path.exists(pth+f+'normal/'):\n",
    "#             os.system('mkdir '+pth+f+'normal/')\n",
    "#         if not os.path.exists(pth+f+'complement/'):\n",
    "#             os.system('mkdir '+pth+f+'complement/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca3b672-42ae-43b5-99a1-ab4b643a519d",
   "metadata": {},
   "source": [
    "# Preprocess and predict Image with respect to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5137a056-9772-41c9-a436-ac24a3aed05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocessImage(model_in_use,im):\n",
    "    if model_in_use=='VGG16':\n",
    "        im=tf.keras.applications.vgg16.preprocess_input(im)\n",
    "        return im\n",
    "    if model_in_use=='InceptionV3':\n",
    "        im=tf.keras.applications.inception_v3.preprocess_input(im)\n",
    "        return im\n",
    "    if model_in_use=='ResNet50':\n",
    "        im=tf.keras.applications.resnet50.preprocess_input(im)\n",
    "        return im\n",
    "\n",
    "def predict_img(model_in_use,im,print_data=True):\n",
    "    model_in_use=model_in_use\n",
    "    predictions = model_in_use(np.array([im]))\n",
    "    prediction_class = np.argmax(predictions[0])\n",
    "    # call_model_args = {class_idx_str: prediction_class}\n",
    "    top10list=np.flip(np.array(predictions[0]).argsort())[:10]\n",
    "    \n",
    "    if print_data == True:\n",
    "        print(\"Prediction class: \" + str(prediction_class))\n",
    "    decoded_predictions = tf.keras.applications.vgg16.decode_predictions(np.array(predictions), top=1000)[0]  # Top 5 predictions\n",
    "    if print_data == True:\n",
    "        for i, (imagenet_id, label, score) in enumerate(decoded_predictions):\n",
    "            if i>=20:\n",
    "                break\n",
    "            print(f\"{i + 1}:{imagenet_id} {label} ({score:.10f})\")\n",
    "    \n",
    "    id_to_name=[0 for xyz in range(1000)]\n",
    "    top1000list=np.flip(np.array(predictions[0]).argsort())\n",
    "    for xyz in range(1000):\n",
    "        id_to_name[top1000list[xyz]]=decoded_predictions[xyz][1]\n",
    "    return(predictions,decoded_predictions,top10list,top1000list,id_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7819474-7509-4d65-a277-c35fc5259669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f743606b-7b3a-40be-bd15-9de9e6f4e5ae",
   "metadata": {},
   "source": [
    "# Displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "887872ca-b008-400f-8877-7ef4e09e2507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ShowImage(im, title='', ax=None):\n",
    "  if ax is None:\n",
    "    plt.figure()\n",
    "  plt.axis('off')\n",
    "  plt.imshow(im)\n",
    "  plt.title(title)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9a6c88-7d0d-45ef-a0d3-d189e7532c41",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eed246a2-3af1-401a-9865-9df6e2d15933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27c30310-883e-4087-a745-164550058ad2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41942177-a310-44dc-9c19-73a335632d56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16b8f839-0c35-4efd-88a2-96c4f97a632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #tempcell\n",
    "\n",
    "# topk=5\n",
    "# available_models=['VGG16','InceptionV3','ResNet50']\n",
    "# model_to_use='VGG16'\n",
    "# model_using,model_wth_pre_soft_max_using=load_model(model_to_use)\n",
    "# # model_in_use.summary()\n",
    "# filepath_array=[]\n",
    "\n",
    "# # filepath_array.append('/Users/nuthan/ME/Feature-visualization-and-attribution/images/Fireboat2-old.jpg')#originalfireboat picture\n",
    "# # filepath_array.append('/Users/nuthan/ME/Feature-visualization-and-attribution/images/val_set/img200/ILSVRC2012_val_00021666.JPEG')#two rhodesianridgebacks\n",
    "# # filepath_array.append('/Users/nuthan/ME/Feature-visualization-and-attribution/images/val_set/img200/ILSVRC2012_val_00021666.JPEG')#two rhodesianridgebacks\n",
    "# # filepath_array.append('/Users/nuthan/ME/Feature-visualization-and-attribution/images/notfireboat.jpg')#removed fireboat from picture\n",
    "# # filepath_array.append('/Users/nuthan/ME/Feature-visualization-and-attribution/images/fireboat_all_varients/n03344393_8534.JPEG')#skyliner in front of fireboat\n",
    "# # filepath_array.append('/Users/nuthan/ME/Feature-visualization-and-attribution/images/fireboat3.jpg')#fireboat in dark\n",
    "# # filepath_array.append('/Users/nuthan/ME/Feature-visualization-and-attribution/images/fireboat_all_varients/n03344393_773.JPEG')#fireboat from validation set\n",
    "# # filepath_array.append('/Users/nuthan/ME/Feature-visualization-and-attribution/images/val_set/img200/ILSVRC2012_val_00001651.JPEG')#two rhodesianridgebacks\n",
    "# filepath_array.append('/Users/nuthan/ME/Feature-visualization-and-attribution/people.jpg')#two rhodesianridgebacks\n",
    "\n",
    "\n",
    "# for file_path in filepath_array:\n",
    "#     start_time = time.time()\n",
    "#     pth='temp/'\n",
    "#     (fig_name,image)=image_load(model_to_use,file_path) #todo load according to model\n",
    "#     pth=pth+model_to_use+'_'+fig_name\n",
    "#     # create_folders(pth) #undo this\n",
    "#     # im = PreprocessImage(model_to_use,image) #todo preprocess according to model\n",
    "#     # ShowImage(image,title=fig_name)\n",
    "#     # (predictions,decoded_predictions,top10list,top1000list,id_to_name)=predict_img(model_using,im,print_data=True)\n",
    "#     # print(top10list)\n",
    "#     # break\n",
    "#     segs=generate_segments(image,pth,redo=False)\n",
    "#     end_time = time.time()\n",
    "#     print('completed for fig_name')\n",
    "#     print('time taken : '+str(end_time-start_time)+' sec')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20dc024-556f-4d3c-ba76-f4924acbd8eb",
   "metadata": {},
   "source": [
    "# Pixel wise attribution --> Integrated gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26dea221-c2c1-461c-bd0a-2361e3bee069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for one output\n",
    "def call_model_function2(model_in_use,images,target_class_idx_list=[554]):\n",
    "    target_class_idx =  target_class_idx_list[0]\n",
    "    images = tf.convert_to_tensor(images)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(images)\n",
    "        # print('starting inference')\n",
    "        output_layer = model_in_use(images)\n",
    "        # print('completed inference')\n",
    "        output_node = output_layer[:,target_class_idx]\n",
    "        # print('starting gradient calculation for the label')\n",
    "        \n",
    "        gradients = np.array(tape.gradient(output_node, images))\n",
    "\n",
    "        # print('completed gradient calculation for the label')\n",
    "        return  gradients\n",
    "\n",
    "#for multiple outputs\n",
    "def call_model_function(images,target_class_idx_list=[554]):\n",
    "    target_class_idx =  target_class_idx_list\n",
    "    images = tf.convert_to_tensor(images)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(images)\n",
    "        # print('starting inference')\n",
    "        output_layer = model2(images)\n",
    "        # print('completed inference')\n",
    "        output_node = tf.gather(output_layer, target_class_idx, axis=1)\n",
    "        # print('starting gradient calculation for the label')\n",
    "        print(output_node.shape)\n",
    "        gradients = []\n",
    "        for i in range(len(target_class_idx)):\n",
    "            gradient = np.array(tape.gradient(output_node[:,i], images))\n",
    "            gradients.append(gradient)\n",
    "            \n",
    "        gradients=np.asarray(gradients)\n",
    "        print(np.sum(gradients))\n",
    "\n",
    "        # print('completed gradient calculation for the label')\n",
    "        return  gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3d3d4a4-10f0-4a96-a737-5c0288a31240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_IG_attr(model_in_use,x_value,target_class_idx_list,batch_size):\n",
    "    print('started IG')\n",
    "    max_min_baselines=True\n",
    "    x_steps=50\n",
    "    target_class_idx_list=target_class_idx_list\n",
    "    if max_min_baselines is True:\n",
    "        x_baselines = []\n",
    "        x_baselines.append(np.min(x_value) * np.ones_like(x_value))\n",
    "        x_baselines.append(np.max(x_value) * np.ones_like(x_value))\n",
    "    else:\n",
    "        x_baselines = []\n",
    "        x_baselines.append(np.zeros_like(x_value))\n",
    "        x_baselines.append(np.ones_like(x_value))\n",
    "    # print(x_baselines)\n",
    "    # print('completed baselining')\n",
    "    grads = []\n",
    "    for x_baseline in x_baselines:\n",
    "        # print('starting IG for Baselinex')\n",
    "        x_diff = x_value - x_baseline\n",
    "        total_gradients = np.zeros_like(x_value, dtype=np.float32)\n",
    "        x_step_batched = []\n",
    "        for alpha in np.linspace(0, 1, x_steps):\n",
    "            x_step = x_baseline + alpha * x_diff\n",
    "            x_step_batched.append(x_step)\n",
    "            # if len(x_step_batched) == batch_size or alpha == 1:\n",
    "            if alpha == 1:\n",
    "                x_step_batched = np.asarray(x_step_batched)\n",
    "                # print('sending to model')\n",
    "                # print(x_step_batched.shape)\n",
    "                \n",
    "                call_model_output = call_model_function2(model_in_use,x_step_batched,target_class_idx_list=target_class_idx_list)\n",
    "        \n",
    "            # self.format_and_check_call_model_output(call_model_output,\n",
    "            #                                         x_step_batched.shape,\n",
    "            #                                         self.expected_keys)\n",
    "        \n",
    "                total_gradients += call_model_output.sum(axis=0)\n",
    "                x_step_batched = []\n",
    "        \n",
    "        grads.append(total_gradients * x_diff / x_steps)\n",
    "    attr = np.mean(grads, axis=0)\n",
    "    print('completed IG calculation')\n",
    "    attr=attr.max(axis=-1) #change here if there is a change in shape\n",
    "    print('picking up max gradients of 3 channels')\n",
    "    return attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffdf6acd-8de7-4fde-b7ce-3ae46c831761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pixelwise_attributions_and_load(model_in_use,topk,top10list,pth,redo=False):\n",
    "\n",
    "    if os.path.exists(pth+'_pwa'+'.pkl') and redo==False:\n",
    "        with open(pth+'_pwa'+'.pkl', 'rb') as file:\n",
    "            pixelwise_attributions=pickle.load(file)\n",
    "        print('loaded pixel wise attributions from file')\n",
    "        return pixelwise_attributions\n",
    "\n",
    "    else:\n",
    "        print('calculating pixel wise attributions')\n",
    "        pixelwise_attributions={}\n",
    "        for i in range(topk):\n",
    "            print('for '+str(top10list[i]))\n",
    "            target_class_idx_list=[top10list[i]]\n",
    "            pixelwise_attributions[top10list[i]]=Calculate_IG_attr(model_in_use,im,target_class_idx_list,batch_size=20)\n",
    "        with open(pth+'_pwa'+'.pkl', 'wb') as file:\n",
    "            pickle.dump(pixelwise_attributions, file)\n",
    "            file.close()\n",
    "        return pixelwise_attributions\n",
    "\n",
    "        # dont save them into pkl file we can save along wirth xrai attribution along with segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4011ac-56f6-4022-b091-eaddc493a241",
   "metadata": {},
   "source": [
    "# segment wise attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb8dfabf-b8f2-44ba-8c8b-161be47076f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _gain_density(mask1, attr, mask2=None):\n",
    "  # Compute the attr density over mask1. If mask2 is specified, compute density\n",
    "  # for mask1 \\ mask2\n",
    "  if mask2 is None:\n",
    "    added_mask = mask1\n",
    "  else:\n",
    "    added_mask = _get_diff_mask(mask1, mask2)\n",
    "  if not np.any(added_mask):\n",
    "    return -np.inf\n",
    "  else:\n",
    "    return attr[added_mask].mean()\n",
    "\n",
    "\n",
    "def _get_diff_mask(add_mask, base_mask):\n",
    "  return np.logical_and(add_mask, np.logical_not(base_mask))\n",
    "\n",
    "\n",
    "def _get_diff_cnt(add_mask, base_mask):\n",
    "  return np.sum(_get_diff_mask(add_mask, base_mask))\n",
    "\n",
    "def _unpack_segs_to_masks(segs):\n",
    "  masks = []\n",
    "  for seg in segs:\n",
    "    for l in range(seg.min(), seg.max() + 1):\n",
    "      masks.append(seg == l)\n",
    "  return masks\n",
    "\n",
    "def _normalize_image(im, value_range, resize_shape=None):\n",
    "  \"\"\"Normalize an image by resizing it and rescaling its values.\n",
    "\n",
    "  Args:\n",
    "      im: Input image.\n",
    "      value_range: [min_value, max_value]\n",
    "      resize_shape: New image shape. Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "      Resized and rescaled image.\n",
    "  \"\"\"\n",
    "  im_max = np.max(im)\n",
    "  im_min = np.min(im)\n",
    "  im = (im - im_min) / (im_max - im_min)\n",
    "  im = im * (value_range[1] - value_range[0]) + value_range[0]\n",
    "  if resize_shape is not None:\n",
    "    im = resize(im,\n",
    "                resize_shape,\n",
    "                order=3,\n",
    "                mode='constant',\n",
    "                preserve_range=True,\n",
    "                anti_aliasing=True)\n",
    "  return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb282f-1531-4848-b6e5-ce3a8ba18ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2636b077-6eaa-4769-9161-636b5aac1fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_XRAI_attr(attr,segs):\n",
    "    output_attr = -np.inf * np.ones(shape=attr.shape, dtype=float)\n",
    "    list_of_masks_for_prediction=[]\n",
    "    n_masks = len(segs)\n",
    "    current_area_perc = 0.0\n",
    "    area_perc_th=1.0\n",
    "    min_pixel_diff=5\n",
    "    current_mask = np.zeros(attr.shape, dtype=bool)\n",
    "    \n",
    "    masks_trace = []\n",
    "    remaining_masks = {ind: mask for ind, mask in enumerate(segs)}\n",
    "    \n",
    "    added_masks_cnt = 1\n",
    "    # While the mask area is less than area_th and remaining_masks is not empty\n",
    "    while current_area_perc <= area_perc_th:\n",
    "      best_gain = -np.inf\n",
    "      best_key = None\n",
    "      remove_key_queue = []\n",
    "      for mask_key in remaining_masks:\n",
    "        mask = remaining_masks[mask_key]\n",
    "        # If mask does not add more than min_pixel_diff to current mask, remove\n",
    "        mask_pixel_diff = _get_diff_cnt(mask, current_mask)\n",
    "        if mask_pixel_diff < min_pixel_diff:\n",
    "          remove_key_queue.append(mask_key)\n",
    "          # if _logger.isEnabledFor(logging.DEBUG):\n",
    "          #   _logger.debug('Skipping mask with pixel difference: {:.3g},'.format(\n",
    "          #       mask_pixel_diff))\n",
    "          continue\n",
    "        gain = _gain_density(mask, attr, mask2=current_mask)\n",
    "        if gain > best_gain:\n",
    "          best_gain = gain\n",
    "          best_key = mask_key\n",
    "      for key in remove_key_queue:\n",
    "        del remaining_masks[key]\n",
    "      if not remaining_masks:\n",
    "        break\n",
    "      added_mask = remaining_masks[best_key]\n",
    "      mask_diff = _get_diff_mask(added_mask, current_mask)\n",
    "      masks_trace.append((mask_diff, best_gain))\n",
    "\n",
    "      list_of_masks_for_prediction.append(best_key)\n",
    "      current_mask = np.logical_or(current_mask, added_mask)\n",
    "      current_area_perc = np.mean(current_mask)\n",
    "      \n",
    "      output_attr[mask_diff] = best_gain\n",
    "      del remaining_masks[best_key]  # delete used key\n",
    "      # plt.imshow(output_attr)\n",
    "      # plt.show()\n",
    "      # if _logger.isEnabledFor(logging.DEBUG):\n",
    "      #   current_attr_sum = np.sum(attr[current_mask])\n",
    "      #   _logger.debug(\n",
    "      #       '{} of {} masks added,'\n",
    "      #       'attr_sum: {}, area: {:.3g}/{:.3g}, {} remaining masks'.format(\n",
    "      #           added_masks_cnt, n_masks, current_attr_sum, current_area_perc,\n",
    "      #           area_perc_th, len(remaining_masks)))\n",
    "      added_masks_cnt += 1\n",
    "    \n",
    "    uncomputed_mask = output_attr == -np.inf\n",
    "    # Assign the uncomputed areas a value such that sum is same as ig\n",
    "    output_attr[uncomputed_mask] = _gain_density(uncomputed_mask, attr)\n",
    "    # masks_trace = [v[0] for v in sorted(masks_trace, key=lambda x: -x[1])]\n",
    "    # if np.any(uncomputed_mask):\n",
    "    #   masks_trace.append(uncomputed_mask)\n",
    "    # if integer_segments:\n",
    "    #   attr_ranks = np.zeros(shape=attr.shape, dtype=int)\n",
    "    #   for i, mask in enumerate(masks_trace):\n",
    "    #     attr_ranks[mask] = i + 1\n",
    "    #   return output_attr, attr_ranks\n",
    "    # else:\n",
    "    #   return output_attr, masks_trace\n",
    "    return output_attr,list_of_masks_for_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c0e0bf1-fe01-44cb-aa21-ce0c1fe8f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_segmentwise_attributions_and_load(pixelwise_attributions,topk,top10list,pth,segs,redo=False):\n",
    "    list_of_masks={}\n",
    "    if os.path.exists(pth+'_swa'+'.pkl') and redo==False:\n",
    "        with open(pth+'_swa'+'.pkl', 'rb') as file:\n",
    "            (segmentwise_attributions,list_of_masks)=pickle.load(file)\n",
    "        print('loaded segment wise attributions from file')\n",
    "        return (segmentwise_attributions,list_of_masks)\n",
    "\n",
    "    else:\n",
    "        print('calculating segment wise attributions')\n",
    "        segmentwise_attributions={}\n",
    "        for i in range(topk):\n",
    "            print('for '+str(top10list[i]))\n",
    "            target_class_idx_list=[top10list[i]]\n",
    "            segmentwise_attributions[top10list[i]],list_of_masks_for_prediction=calculate_XRAI_attr(pixelwise_attributions[top10list[i]],segs)\n",
    "            # print(list_of_masks_for_prediction)\n",
    "            list_of_masks[top10list[i]]=list_of_masks_for_prediction\n",
    "\n",
    "        with open(pth+'_swa'+'.pkl', 'wb') as file:\n",
    "            pickle.dump((segmentwise_attributions,list_of_masks), file)\n",
    "            file.close()\n",
    "        return (segmentwise_attributions,list_of_masks)\n",
    "\n",
    "        # dont save them into pkl file we can save along wirth xrai attribution along with segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b614df-8c67-4d67-8ffa-ade54f792603",
   "metadata": {},
   "source": [
    "# apply mask on image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58d55d77-9b3f-47df-a8f4-0727377fdfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mask_on_image(im , mask, masking_type):\n",
    "    new_pic=[]\n",
    "    new_im=np.transpose(im, (2, 0, 1))\n",
    "    if masking_type=='normal/':\n",
    "        for i in new_im:\n",
    "            new_pic.append(i*(~mask))\n",
    "    else:\n",
    "        for i in new_im:\n",
    "            new_pic.append(i*(mask))\n",
    "    new_im=np.transpose(new_pic, (1,2,0))\n",
    "    return new_im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94410c8-4db7-4de0-b94c-4abe4b4f107b",
   "metadata": {},
   "source": [
    "# Redaction and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7f0bec-ab9e-464b-b2e3-5f9daaf6ec4e",
   "metadata": {},
   "source": [
    "## calculating predictions for each redaction -variation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "301b776e-6be5-40d1-b7fe-c11a416ce225",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_predictions_for_each_redaction(model_in_use,segmentwise_attributions,image,im,topk,top10list,prediction_id,list_of_masks,new_segs,predictions):    \n",
    "    top_heatmaps=100\n",
    "    masking_type='normal/'\n",
    "    dict1={}\n",
    "    dict2={}\n",
    "    dict_for_redacted_images=[]\n",
    "    all_predictions_after_this_redaction=[]\n",
    "    for i in top10list:\n",
    "        dict1[i]=[]\n",
    "        dict2[i]=[]\n",
    "        dict1[i].append(predictions[0][i].numpy())\n",
    "        # dict2[i].append(100.0)\n",
    "    numbered_masks = {ind: mask for ind, mask in enumerate(new_segs)}\n",
    "    oldmask=np.zeros(numbered_masks[0].shape, dtype=bool)\n",
    "    all_predictions_after_this_redaction.append([])\n",
    "    # for i in top10list:\n",
    "    #         dict1[i].append(predictions1[0][i].numpy())\n",
    "    \n",
    "\n",
    "    # while top_heatmaps>=0:\n",
    "    #     # for top_heatmaps in range(100,-0.9,-0.1)\n",
    "    #     mask = segmentwise_attributions[prediction_id] > np.percentile(segmentwise_attributions[prediction_id], top_heatmaps)\n",
    "    #     new_im=mask_on_image(im , mask, masking_type)\n",
    "    #     new_im1=mask_on_image(image , mask, masking_type)\n",
    "    #     (predictions1,decoded_predictions1,top10list1,top1000list1,id_to_name1)=predict_img(model_in_use,new_im,print_data=False)\n",
    "    #     dict_for_redacted_images.append(new_im1)\n",
    "    #     for i in top10list:\n",
    "    #         dict1[i].append(predictions1[0][i].numpy())\n",
    "    #     top_heatmaps=top_heatmaps-1\n",
    "\n",
    "    for mask_id in list_of_masks[prediction_id]:\n",
    "        # for top_heatmaps in range(100,-0.9,-0.1)\n",
    "        mask=numbered_masks[mask_id]\n",
    "        mask=np.logical_or(mask,oldmask)\n",
    "        new_im=mask_on_image(im , mask, masking_type)\n",
    "        new_im1=mask_on_image(image , mask, masking_type)\n",
    "        oldmask=mask\n",
    "        # plt.imshow(mask)\n",
    "        # plt.show()\n",
    "        \n",
    "        (predictions1,decoded_predictions1,top10list1,top1000list1,id_to_name1)=predict_img(model_in_use,new_im,print_data=False)\n",
    "        dict_for_redacted_images.append(new_im1)\n",
    "        all_predictions_after_this_redaction.append(predictions1[0].numpy())\n",
    "        for i in top10list:\n",
    "            dict1[i].append(predictions1[0][i].numpy())\n",
    "\n",
    "    # plt.imshow(mask)\n",
    "    # plt.show()\n",
    "    \n",
    "    for key in dict1:\n",
    "        for k in range(len(dict1[key])):                \n",
    "            dict2[key].append((dict1[key][k]/dict1[key][0])*100)\n",
    "    return [dict1,dict2,dict_for_redacted_images,all_predictions_after_this_redaction]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5d8ed7b-3ccc-47b8-b423-cc5978b459df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_predictions_by_redactions(model_in_use,segmentwise_attributions,image,im,topk,top10list,pth,list_of_masks,new_segs,predictions,redo=False,dont_save=False):\n",
    "\n",
    "    if os.path.exists(pth+'_redactions'+'.pkl') and redo==False:\n",
    "        with open(pth+'_redactions'+'.pkl', 'rb') as file:\n",
    "            x=pickle.load(file)\n",
    "            # [final_prediction_dictionary,final_prediction_dictionary_with_percentages,final_redacted_images]=pickle.load(file)\n",
    "        print('loaded final predictions after redactions from file for type1 check')\n",
    "        return x\n",
    "        # return final_prediction_dictionary,final_prediction_dictionary_with_percentages,final_redacted_images,all_predictions_after_redaction\n",
    "\n",
    "    else:\n",
    "        all_predictions_after_redaction={}\n",
    "        final_prediction_dictionary={}\n",
    "        final_prediction_dictionary_with_percentages={}\n",
    "        final_redacted_images={}\n",
    "        for prediction_id in top10list[:topk]:\n",
    "            print('loading predictions while redating '+str(prediction_id))\n",
    "            prediction_dictionary,prediction_dictionary_with_percentages,dict_for_redacted_images,all_predictions_after_this_redaction=calculate_predictions_for_each_redaction(model_in_use,segmentwise_attributions,image,im,topk,top10list,prediction_id,list_of_masks,new_segs,predictions)\n",
    "            final_prediction_dictionary[prediction_id]=prediction_dictionary\n",
    "            final_prediction_dictionary_with_percentages[prediction_id]=prediction_dictionary_with_percentages\n",
    "            all_predictions_after_redaction[prediction_id]=all_predictions_after_this_redaction\n",
    "            # final_redacted_images[prediction_id]=dict_for_redacted_images # undo this if you are not using function to calcluate redaction images\n",
    "\n",
    "        if dont_save== False:\n",
    "            with open(pth+'_redactions'+'.pkl', 'wb') as file:\n",
    "                pickle.dump([final_prediction_dictionary,final_prediction_dictionary_with_percentages,final_redacted_images,all_predictions_after_redaction], file)\n",
    "                file.close()\n",
    "\n",
    "        return [final_prediction_dictionary,final_prediction_dictionary_with_percentages,final_redacted_images,all_predictions_after_redaction]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f22475-bcdf-4ef8-8d92-64bf564ffff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acfa3b2f-a852-409d-9128-be10773918de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_redacted_image(segmentwise_attributions,prediction_id,redaction1_percentile):\n",
    "    masking_type='normal/'\n",
    "    mask = segmentwise_attributions[prediction_id] > np.percentile(segmentwise_attributions[prediction_id], 100-redaction1_percentile)\n",
    "    new_im1=mask_on_image(image , mask, masking_type)\n",
    "    return new_im1,mask\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087bc173-5c56-4895-96a1-0c6e8dbabf47",
   "metadata": {},
   "source": [
    "# Type-1 and Type-2 check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2515a668-54c5-4360-8da1-b820c606b69e",
   "metadata": {},
   "source": [
    "## plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7898ff91-fb3b-4cd2-a4a2-05948fc3bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_for_type1_redaction1(redacted_image_with_pred1,redacted_mask_with_pred1,final_prediction_dictionary,first_redaction,id_to_name,prediction1,prediction2,redaction1_percentile,type_of_graph):\n",
    "    ROWS=1\n",
    "    COLS=2\n",
    "    UPSCALE_FACTOR = 10\n",
    "    fig, axs = plt.subplots(ROWS, COLS, figsize=(ROWS * UPSCALE_FACTOR, COLS * (UPSCALE_FACTOR-5)))\n",
    "    axs.flat[0].imshow(redacted_image_with_pred1)\n",
    "    axs.flat[1].imshow(redacted_mask_with_pred1)\n",
    "    \n",
    "    axs.flat[0].axis('off')\n",
    "    axs.flat[1].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    ROWS=1\n",
    "    COLS=2\n",
    "    UPSCALE_FACTOR = 10\n",
    "    fig, axs = plt.subplots(ROWS, COLS, figsize=(ROWS * UPSCALE_FACTOR, COLS * (UPSCALE_FACTOR-8)))\n",
    "    # for xy in list(final_prediction_dictionary[prediction1].keys())[:5]:\n",
    "    #     axs.flat[0].plot(range(len(final_prediction_dictionary[prediction1][xy])),final_prediction_dictionary[prediction1][xy],label=id_to_name[xy])\n",
    "    axs.flat[0].plot(range(len(final_prediction_dictionary[prediction1][prediction1])),final_prediction_dictionary[prediction1][prediction1],label=id_to_name[prediction1])\n",
    "    axs.flat[0].plot(range(len(final_prediction_dictionary[prediction1][prediction2])),final_prediction_dictionary[prediction1][prediction2],label=id_to_name[prediction2])\n",
    "    axs.flat[0].legend()\n",
    "    if type_of_graph == 'prob':\n",
    "        axs.flat[0].set_title('probabilities vs redactions')\n",
    "        axs.flat[0].set_ylabel('probabilities')\n",
    "        axs.flat[0].set_xlabel('redactions')\n",
    "    elif type_of_graph == 'pre_sm':\n",
    "        axs.flat[0].set_title('pre_softmax vs redactions')\n",
    "        axs.flat[0].set_ylabel('pre_softmax')\n",
    "        axs.flat[0].set_xlabel('redactions')\n",
    "    axs.flat[0].axvline(x=redaction1_percentile, color='r', linestyle=':',label=str(redaction1_percentile))\n",
    "    # for xy in list(first_redaction.keys())[:5]:\n",
    "    #     axs.flat[1].plot(range(len(first_redaction[xy])),first_redaction[xy],label=id_to_name[xy])\n",
    "    axs.flat[1].plot(range(len(first_redaction[prediction1])),first_redaction[prediction1],label=id_to_name[prediction1])\n",
    "    axs.flat[1].plot(range(len(first_redaction[prediction2])),first_redaction[prediction2],label=id_to_name[prediction2])\n",
    "    axs.flat[1].legend()\n",
    "    axs.flat[1].set_title('percentages vs redactions')\n",
    "    axs.flat[1].set_ylabel('percentages')\n",
    "    axs.flat[1].set_xlabel('redactions')\n",
    "    axs.flat[1].axvline(x=redaction1_percentile, color='r', linestyle=':',label=str(redaction1_percentile))\n",
    "    plt.show()\n",
    "\n",
    "def plot_for_type1_redaction2(redacted_image_with_pred2,redacted_mask_with_pred2,final_prediction_dictionary,second_redaction,id_to_name,prediction1,prediction2,redaction2_percentile):\n",
    "\n",
    "\n",
    "    ROWS=1\n",
    "    COLS=2\n",
    "    UPSCALE_FACTOR = 10\n",
    "    fig, axs = plt.subplots(ROWS, COLS, figsize=(ROWS * UPSCALE_FACTOR, COLS * (UPSCALE_FACTOR-5)))\n",
    "    axs.flat[0].imshow(redacted_image_with_pred2)\n",
    "    axs.flat[1].imshow(redacted_mask_with_pred2)\n",
    "    axs.flat[0].axis('off')\n",
    "    axs.flat[1].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    ROWS=1\n",
    "    COLS=2\n",
    "    UPSCALE_FACTOR = 10\n",
    "    fig, axs = plt.subplots(ROWS, COLS, figsize=(ROWS * UPSCALE_FACTOR, COLS * (UPSCALE_FACTOR-8)))\n",
    "    # for xy in list(final_prediction_dictionary[prediction2].keys())[:5]:\n",
    "    #     axs.flat[0].plot(range(len(final_prediction_dictionary[prediction2][xy])),final_prediction_dictionary[prediction2][xy],label=id_to_name[xy])\n",
    "\n",
    "    axs.flat[0].plot(range(len(final_prediction_dictionary[prediction2][prediction1])),final_prediction_dictionary[prediction2][prediction1],label=id_to_name[prediction1])\n",
    "    axs.flat[0].plot(range(len(final_prediction_dictionary[prediction2][prediction2])),final_prediction_dictionary[prediction2][prediction2],label=id_to_name[prediction2])\n",
    "    axs.flat[0].legend()\n",
    "    axs.flat[0].set_title('probabilities vs redaction percentile')\n",
    "    axs.flat[0].set_ylabel('probabilities')\n",
    "    axs.flat[0].set_xlabel('redaction percentile')\n",
    "    axs.flat[0].axvline(x=redaction2_percentile, color='r', linestyle=':',label=str(redaction2_percentile))\n",
    "    # for xy in list(second_redaction.keys())[:5]:\n",
    "    #     axs.flat[1].plot(range(len(second_redaction[xy])),second_redaction[xy],label=id_to_name[xy])\n",
    "\n",
    "    axs.flat[1].plot(range(len(second_redaction[prediction1])),second_redaction[prediction1],label=id_to_name[prediction1])\n",
    "    axs.flat[1].plot(range(len(second_redaction[prediction2])),second_redaction[prediction2],label=id_to_name[prediction2])\n",
    "    axs.flat[1].legend()\n",
    "    axs.flat[1].set_title('percentages vs redaction percentile')\n",
    "    axs.flat[1].set_ylabel('percentages')\n",
    "    axs.flat[1].set_xlabel('redaction percentile')\n",
    "    axs.flat[1].axvline(x=redaction2_percentile, color='r', linestyle=':',label=str(redaction2_percentile))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_for_type2_sequence_of_masks(segmentwise_attributions,prediction1,prediction2,redaction1_percentile,redaction2_percentile):\n",
    "    xy=segmentwise_attributions[prediction1] > np.percentile(segmentwise_attributions[prediction1], 100)\n",
    "    mask_temp=np.zeros(xy.shape)\n",
    "    mask_temp_array=[]\n",
    "    mask_temp_array2=[]\n",
    "    for o in range(redaction1_percentile+1):\n",
    "        mask_temp_new = segmentwise_attributions[prediction1] > np.percentile(segmentwise_attributions[prediction1], 100-o)\n",
    "        if np.all(mask_temp_new== mask_temp):\n",
    "            continue\n",
    "        else:\n",
    "            mask_temp= mask_temp_new\n",
    "            mask_temp_array.append(mask_temp_new)\n",
    "            mask_temp_array2.append(o)\n",
    "   \n",
    "    n=len(mask_temp_array)                \n",
    "    # n=redaction1_percentile+1\n",
    "    ROWS = math.floor(math.sqrt(n))\n",
    "    COLS = math.ceil(n / ROWS)\n",
    "    UPSCALE_FACTOR = 10\n",
    "    fig, axs = plt.subplots(ROWS, COLS, figsize=(ROWS * UPSCALE_FACTOR, COLS * (UPSCALE_FACTOR-5)))\n",
    "    for o in range(len(mask_temp_array)):\n",
    "        axs.flat[o].imshow(mask_temp_array[o])\n",
    "        axs.flat[o].set_title(mask_temp_array2[o])\n",
    "    # for o in range(redaction1_percentile+1):\n",
    "    #     mask_temp = segmentwise_attributions[prediction1] > np.percentile(segmentwise_attributions[prediction1], 100-o)\n",
    "    #     axs.flat[o].imshow(mask_temp)\n",
    "    plt.show()\n",
    "    \n",
    "    xy=segmentwise_attributions[prediction1] > np.percentile(segmentwise_attributions[prediction1], 100)\n",
    "    mask_temp=np.zeros(xy.shape)\n",
    "    mask_temp_array=[]\n",
    "    mask_temp_array2=[]\n",
    "    for o in range(redaction2_percentile+1):\n",
    "        mask_temp_new = segmentwise_attributions[prediction2] > np.percentile(segmentwise_attributions[prediction2], 100-o)\n",
    "        if np.all(mask_temp_new== mask_temp):\n",
    "            continue\n",
    "        else:\n",
    "            mask_temp= mask_temp_new\n",
    "            mask_temp_array.append(mask_temp_new)\n",
    "            mask_temp_array2.append(o)\n",
    "    \n",
    "    \n",
    "    n=len(mask_temp_array)                \n",
    "    # n=redaction1_percentile+1\n",
    "    ROWS = math.floor(math.sqrt(n))\n",
    "    COLS = math.ceil(n / ROWS)\n",
    "    UPSCALE_FACTOR = 10\n",
    "    fig, axs = plt.subplots(ROWS, COLS, figsize=(ROWS * UPSCALE_FACTOR, COLS * (UPSCALE_FACTOR-5)))\n",
    "    for o in range(len(mask_temp_array)):\n",
    "        axs.flat[o].imshow(mask_temp_array[o])\n",
    "        axs.flat[o].set_title(mask_temp_array2[o])\n",
    "    # for o in range(redaction1_percentile+1):\n",
    "    #     mask_temp = segmentwise_attributions[prediction1] > np.percentile(segmentwise_attributions[prediction1], 100-o)\n",
    "    #     axs.flat[o].imshow(mask_temp)\n",
    "    plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0537e2d0-6cc8-4c3a-aa1a-27ac171ac99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate mask using list of all mask ids\n",
    "def get_mask(mask_id_intersection,new_segs):\n",
    "    numbered_masks = {ind: mask for ind, mask in enumerate(new_segs)}\n",
    "    oldmask=np.zeros(numbered_masks[0].shape, dtype=bool)\n",
    "    for i in mask_id_intersection:\n",
    "        mask=np.logical_or(numbered_masks[i],oldmask)\n",
    "        # plt.imshow(mask)\n",
    "        # plt.show()\n",
    "        oldmask=mask\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d078db8-e12c-4d90-a621-fb1898154716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf127135-d30e-491c-a258-e32dda84e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if not required\n",
    "\n",
    "\n",
    "def check_for_type_1_and_2_using_rankings_exp1(segs,pixelwise_attributions,segmentwise_attributions,final_prediction_dictionary,final_prediction_dictionary_with_percentages,final_redacted_images,model_in_use,im,image,topk,top10list,delta_for_type1,checkonly=None):\n",
    "    combs=[i for i in range(topk)]\n",
    "    cnt=0\n",
    "    for (x1,x2) in list(combinations(combs, 2)):\n",
    "        prediction1=top10list[x1]\n",
    "        prediction2=top10list[x2]\n",
    "        first_redaction=final_prediction_dictionary_with_percentages[prediction1]\n",
    "        second_redaction=final_prediction_dictionary_with_percentages[prediction2]\n",
    "        print(x1,x2)\n",
    "        print(id_to_name[prediction1],id_to_name[prediction2])\n",
    "        print(prediction1,prediction2)\n",
    "        type1_flag=None\n",
    "        type2_flag=None\n",
    "\n",
    "        \n",
    "        \n",
    "        '''if checkonly=='type1' or checkonly==None:\n",
    "            # checking for type1\n",
    "            print('checking for type1')\n",
    "            for i in range(len(first_redaction[prediction1])):\n",
    "                flag1=None\n",
    "                if first_redaction[prediction1][i] <= 100-delta_for_type1 and first_redaction[prediction2][i] >= delta_for_type1:\n",
    "                    flag1=True\n",
    "                    redaction1_percentile=i\n",
    "                    break\n",
    "        \n",
    "            for i in range(len(first_redaction[prediction1])):\n",
    "                flag2=None    \n",
    "                if second_redaction[prediction2][i] <= 100-delta_for_type1 and second_redaction[prediction1][i] >= delta_for_type1:\n",
    "                    flag2=True\n",
    "                    redaction2_percentile=i\n",
    "                    break\n",
    "        \n",
    "            if flag1==True and flag2==True:\n",
    "                type1_flag=True\n",
    "                cnt=cnt+1\n",
    "                print('Yes its type1')\n",
    "                redacted_image_with_pred1,redacted_mask_with_pred1=show_redacted_image(segmentwise_attributions,prediction1,redaction1_percentile)\n",
    "                redacted_image_with_pred2,redacted_mask_with_pred2=show_redacted_image(segmentwise_attributions,prediction2,redaction2_percentile)\n",
    "        \n",
    "                \n",
    "                print('redaction for '+str(prediction1)+' percentile at ',redaction1_percentile)\n",
    "                print(' Initial predcition of '+str(prediction1),final_prediction_dictionary[prediction1][prediction1][0])\n",
    "                print(' final predcition of '+str(prediction1),final_prediction_dictionary[prediction1][prediction1][redaction1_percentile])\n",
    "                print(' final percentage of '+str(prediction1),first_redaction[prediction1][redaction1_percentile])\n",
    "                print(' Initial predcition of '+str(prediction2),final_prediction_dictionary[prediction1][prediction2][0])\n",
    "                print(' final predcition of '+str(prediction2),final_prediction_dictionary[prediction1][prediction2][redaction1_percentile])\n",
    "                print(' final percentage of '+str(prediction2),first_redaction[prediction2][redaction1_percentile])\n",
    "    \n",
    "    \n",
    "                \n",
    "                plot_for_type1_redaction1(redacted_image_with_pred1,redacted_mask_with_pred1,final_prediction_dictionary,first_redaction,id_to_name,prediction1,prediction2,redaction1_percentile)\n",
    "    \n",
    "                # print(len(first_redaction[prediction1]))\n",
    "                \n",
    "                print('redaction for '+str(prediction2)+' percentile at ',redaction2_percentile)\n",
    "                print(' Initial predcition of '+str(prediction2),final_prediction_dictionary[prediction2][prediction2][0])\n",
    "                print(' final predcition of '+str(prediction2),final_prediction_dictionary[prediction2][prediction2][redaction2_percentile])\n",
    "                print(' final percentage of '+str(prediction2),second_redaction[prediction2][redaction2_percentile])\n",
    "                print(' Initial predcition of '+str(prediction1),final_prediction_dictionary[prediction2][prediction1][0])\n",
    "                print(' final predcition of '+str(prediction1),final_prediction_dictionary[prediction2][prediction1][redaction2_percentile])\n",
    "                print(' final percentage of '+str(prediction1),second_redaction[prediction1][redaction2_percentile])\n",
    "    \n",
    "                \n",
    "                plot_for_type1_redaction2(redacted_image_with_pred2,redacted_mask_with_pred2,final_prediction_dictionary,second_redaction,id_to_name,prediction1,prediction2,redaction2_percentile)\n",
    "    \n",
    "                \n",
    "                mask_for_redaction1 = segmentwise_attributions[prediction1] > np.percentile(segmentwise_attributions[prediction1], 100-redaction1_percentile)\n",
    "                mask_for_redaction2 = segmentwise_attributions[prediction2] > np.percentile(segmentwise_attributions[prediction2], 100-redaction2_percentile)\n",
    "        \n",
    "                \n",
    "                \n",
    "                # print(id_to_name[prediction1])\n",
    "                # plt.imshow(redacted_image_with_pred1)\n",
    "                # plt.show()\n",
    "                # print(id_to_name[prediction2])\n",
    "                # plt.imshow(redacted_image_with_pred2)\n",
    "                # plt.show()'''\n",
    "\n",
    "\n",
    "        if checkonly=='type2' or checkonly==None and type1_flag!=True:\n",
    "            #checking for type2\n",
    "            print('checking for type2')\n",
    "            # print(first_redaction[prediction1])\n",
    "            # print(first_redaction[prediction2])\n",
    "            for i in range(len(first_redaction[prediction1])):\n",
    "                flag1=None\n",
    "                # print(i,first_redaction[prediction1][i],first_redaction[prediction2][i])\n",
    "                if first_redaction[prediction1][i] <= 20 and first_redaction[prediction2][i] <= 20:\n",
    "                    flag1=True\n",
    "                    redaction1_percentile=i\n",
    "                    break\n",
    "        \n",
    "            for i in range(len(first_redaction[prediction1])):\n",
    "                flag2=None\n",
    "                # print(i,second_redaction[prediction2][i],second_redaction[prediction1][i])\n",
    "        \n",
    "                if second_redaction[prediction2][i] <= 20 and second_redaction[prediction1][i] <= 20:\n",
    "                    flag2=True\n",
    "                    redaction2_percentile=i\n",
    "                    break\n",
    "            # print(flag1,flag2)\n",
    "            if flag1==True and flag2==True:\n",
    "                \n",
    "                # mask_for_redaction1 = segmentwise_attributions[prediction1] > np.percentile(segmentwise_attributions[prediction1], 100-redaction1_percentile)\n",
    "                # mask_for_redaction2 = segmentwise_attributions[prediction2] > np.percentile(segmentwise_attributions[prediction2], 100-redaction2_percentile)\n",
    "    \n",
    "                # plt.imshow(mask_for_redaction1)\n",
    "                # plt.show()\n",
    "                # plt.imshow(new_im1)\n",
    "                # plt.show()\n",
    "                # plt.imshow(mask_for_redaction2)\n",
    "                # plt.show()\n",
    "                # plt.imshow(new_im1)\n",
    "                # plt.show()\n",
    "                mask_union=np.logical_or(mask_for_redaction1, mask_for_redaction2) \n",
    "                mask_intersection=np.logical_and(mask_for_redaction1, mask_for_redaction2) \n",
    "                mask_only_for_1=(mask_union.astype(float)-mask_for_redaction2.astype(float)).astype(bool)\n",
    "                mask_only_for_2=(mask_union.astype(float)-mask_for_redaction1.astype(float)).astype(bool)\n",
    "                \n",
    "                \n",
    "        \n",
    "                redacted_image_with_pred1,redacted_mask_with_pred1=show_redacted_image(segmentwise_attributions,prediction1,redaction1_percentile)\n",
    "                redacted_image_with_pred2,redacted_mask_with_pred2=show_redacted_image(segmentwise_attributions,prediction2,redaction2_percentile)\n",
    "                \n",
    "                \n",
    "                masking_type='normal/'\n",
    "                new_im=mask_on_image(im , mask_only_for_1, masking_type)\n",
    "                new_im1=mask_on_image(image , mask_only_for_1, masking_type)\n",
    "                (predictions1,decoded_predictions1,top10list1,top1000list1,id_to_name1)=predict_img(model_in_use,new_im,print_data=False)\n",
    "                new_prediction1_percentage_after_only1=(predictions1[0][prediction1].numpy()/final_prediction_dictionary[prediction1][prediction1][0])*100\n",
    "                new_prediction2_percentage_after_only1=(predictions1[0][prediction2].numpy()/final_prediction_dictionary[prediction2][prediction2][0])*100\n",
    "                \n",
    "                # plt.imshow(mask_only_for_1)\n",
    "                # plt.show()\n",
    "                # plt.imshow(new_im1)\n",
    "                # plt.show()\n",
    "                masking_type='normal/'\n",
    "                new_im=mask_on_image(im , mask_only_for_2, masking_type)\n",
    "                new_im1=mask_on_image(image , mask_only_for_2, masking_type)\n",
    "                (predictions2,decoded_predictions2,top10list2,top1000list2,id_to_name2)=predict_img(model_in_use,new_im,print_data=False)\n",
    "                new_prediction1_percentage_after_only2=(predictions2[0][prediction1].numpy()/final_prediction_dictionary[prediction1][prediction1][0])*100\n",
    "                new_prediction2_percentage_after_only2=(predictions2[0][prediction2].numpy()/final_prediction_dictionary[prediction2][prediction2][0])*100\n",
    "                # plt.imshow(mask_only_for_2)\n",
    "                # plt.show()\n",
    "                \n",
    "                masking_type='normal/'\n",
    "                new_im=mask_on_image(im , mask_intersection, masking_type)\n",
    "                new_im1=mask_on_image(image , mask_intersection, masking_type)\n",
    "                (predictions3,decoded_predictions3,top10list3,top1000list3,id_to_name3)=predict_img(model_in_use,new_im,print_data=False)\n",
    "                new_prediction1_percentage_after_intersection=(predictions3[0][prediction1].numpy()/final_prediction_dictionary[prediction1][prediction1][0])*100\n",
    "                new_prediction2_percentage_after_intersection=(predictions3[0][prediction2].numpy()/final_prediction_dictionary[prediction2][prediction2][0])*100\n",
    "                \n",
    "                # if True:\n",
    "                if new_prediction1_percentage_after_only1 > 70 and new_prediction2_percentage_after_only1 > 70 and new_prediction1_percentage_after_only2 > 70 and new_prediction2_percentage_after_only2 >70 :\n",
    "                    print('Yes its type2')\n",
    "                    type2_flag=True\n",
    "                    print('redaction for '+str(prediction1)+' percentile at ',redaction1_percentile)\n",
    "                    print(' Initial predcition of '+str(prediction1),final_prediction_dictionary[prediction1][prediction1][0])\n",
    "                    print(' final predcition of '+str(prediction1),final_prediction_dictionary[prediction1][prediction1][redaction1_percentile])\n",
    "                    print(' final percentage of '+str(prediction1),first_redaction[prediction1][redaction1_percentile])\n",
    "                    print(' Initial predcition of '+str(prediction2),final_prediction_dictionary[prediction1][prediction2][0])\n",
    "                    print(' final predcition of '+str(prediction2),final_prediction_dictionary[prediction1][prediction2][redaction1_percentile])\n",
    "                    print(' final percentage of '+str(prediction2),first_redaction[prediction2][redaction1_percentile])\n",
    "    \n",
    "                    plot_for_type1_redaction1(redacted_image_with_pred1,redacted_mask_with_pred1,final_prediction_dictionary,first_redaction,id_to_name,prediction1,prediction2,redaction1_percentile)\n",
    "    \n",
    "                    \n",
    "                    print('redaction for '+str(prediction2)+' percentile at ',redaction2_percentile)\n",
    "                    print(' Initial predcition of '+str(prediction2),final_prediction_dictionary[prediction2][prediction2][0])\n",
    "                    print(' final predcition of '+str(prediction2),final_prediction_dictionary[prediction2][prediction2][redaction2_percentile])\n",
    "                    print(' final percentage of '+str(prediction2),second_redaction[prediction2][redaction2_percentile])\n",
    "                    print(' Initial predcition of '+str(prediction1),final_prediction_dictionary[prediction2][prediction1][0])\n",
    "                    print(' final predcition of '+str(prediction1),final_prediction_dictionary[prediction2][prediction1][redaction2_percentile])\n",
    "                    print(' final percentage of '+str(prediction1),second_redaction[prediction1][redaction2_percentile])\n",
    "    \n",
    "                    plot_for_type1_redaction2(redacted_image_with_pred2,redacted_mask_with_pred2,final_prediction_dictionary,second_redaction,id_to_name,prediction1,prediction2,redaction2_percentile)\n",
    "    \n",
    "                    print('redaction only for '+str(prediction1)+' but not for '+str(prediction2))\n",
    "                    print(' Initial predcition of '+str(prediction1),final_prediction_dictionary[prediction1][prediction1][0])\n",
    "                    print(' final predcition of '+str(prediction1),predictions1[0][prediction1].numpy())\n",
    "                    print(' final percentage of '+str(prediction1),new_prediction1_percentage_after_only1)\n",
    "                    print(' Initial predcition of '+str(prediction2),final_prediction_dictionary[prediction2][prediction2][0])\n",
    "                    print(' final predcition of '+str(prediction2),predictions1[0][prediction2].numpy())\n",
    "                    print(' final percentage of '+str(prediction2),new_prediction2_percentage_after_only1)\n",
    "        \n",
    "                    print('redaction only for '+str(prediction2)+' but not for '+str(prediction1))\n",
    "                    print(' Initial predcition of '+str(prediction2),final_prediction_dictionary[prediction2][prediction2][0])\n",
    "                    print(' final predcition of '+str(prediction2),predictions2[0][prediction2].numpy())\n",
    "                    print(' final percentage of '+str(prediction2),new_prediction2_percentage_after_only2)\n",
    "                    print(' Initial predcition of '+str(prediction1),final_prediction_dictionary[prediction1][prediction1][0])\n",
    "                    print(' final predcition of '+str(prediction1),predictions2[0][prediction1].numpy())\n",
    "                    print(' final percentage of '+str(prediction1),new_prediction1_percentage_after_only2)\n",
    "        \n",
    "                    print('redaction of intersection of '+str(prediction1)+' and '+str(prediction2))\n",
    "                    print(' Initial predcition of '+str(prediction1),final_prediction_dictionary[prediction1][prediction1][0])\n",
    "                    print(' final predcition of '+str(prediction1),predictions3[0][prediction1].numpy())\n",
    "                    print(' final percentage of '+str(prediction1),new_prediction1_percentage_after_intersection)\n",
    "                    print(' Initial predcition of '+str(prediction2),final_prediction_dictionary[prediction2][prediction2][0])\n",
    "                    print(' final predcition of '+str(prediction2),predictions3[0][prediction2].numpy())\n",
    "                    print(' final percentage of '+str(prediction2),new_prediction2_percentage_after_intersection)\n",
    "    \n",
    "                    plot_for_type2_sequence_of_masks(segmentwise_attributions,prediction1,prediction2,redaction1_percentile,redaction2_percentile)\n",
    "    \n",
    "                    \n",
    "                    \n",
    "                # plt.imshow(mask_only_for_2)\n",
    "                # plt.show()\n",
    "                # plt.imshow(new_im1)\n",
    "                # plt.show()\n",
    "                # print(id_to_name[prediction1])\n",
    "                # plt.imshow(redacted_image_with_pred1)\n",
    "                # plt.show()\n",
    "                # plt.imshow(mask_for_redaction1)\n",
    "                # plt.show()\n",
    "                # # plt.imshow(mask_only_for_1)\n",
    "                # # plt.show()\n",
    "                # print(id_to_name[prediction2])\n",
    "                # plt.imshow(redacted_image_with_pred2)\n",
    "                # plt.show()\n",
    "                # plt.imshow(mask_for_redaction2)\n",
    "                # plt.show()\n",
    "                # # plt.imshow(mask_only_for_2)\n",
    "                # # plt.show()\n",
    "                # plt.imshow(mask_union)\n",
    "                # plt.show()\n",
    "                # plt.imshow(mask_intersection)\n",
    "                # plt.show()'''\n",
    "        \n",
    "        # break\n",
    "    if type1_flag==True and type2_flag==True:\n",
    "        print('Here i am , Both are true')\n",
    "    print(cnt)\n",
    "    return cnt\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c8e5b19-d6c4-40ae-88ef-bd4bee0aa345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for disjoint experiment \n",
    "def rank_the_disjoint_segments_based_on_predictions(list_of_masks,pred1,pred2,new_segs):\n",
    "    a=list_of_masks[pred1]\n",
    "    b=list_of_masks[pred2]\n",
    "    \n",
    "    ranked_a=[]\n",
    "    ranked_b=[]\n",
    "    ranked_a_list=[]\n",
    "    ranked_b_list=[]\n",
    "    adjacent_segs={}\n",
    "    \n",
    "    for i in range(len(a)):\n",
    "        if a[i]!=b[i]:\n",
    "            if a[i] not in ranked_b:\n",
    "                ranked_a.append(a[i])\n",
    "                ranked_a_list.append({'mask_id':a[i],'required_for_pred':True})\n",
    "            else:\n",
    "                ranked_a_list.append({'mask_id':a[i],'required_for_pred':False})\n",
    "            if b[i] not in ranked_a:\n",
    "                ranked_b.append(b[i])\n",
    "                ranked_b_list.append({'mask_id':b[i],'required_for_pred':True})\n",
    "            else:\n",
    "                ranked_b_list.append({'mask_id':b[i],'required_for_pred':False})\n",
    "        else:\n",
    "            adjacent_segs[a[i]]=get_adjacent_segments(new_segs,a[i])\n",
    "            #remove these once you figure out how to check for adjacent segments\n",
    "            # ranked_a.append(a[i])\n",
    "            # ranked_b.append(b[i])\n",
    "            ranked_a_list.append({'mask_id':a[i],'required_for_pred':'equal'})\n",
    "            ranked_b_list.append({'mask_id':b[i],'required_for_pred':'equal'})\n",
    "    \n",
    "    \n",
    "    for i in adjacent_segs.keys():\n",
    "        for j in ranked_a_list:\n",
    "            if i == j['mask_id'] and j['required_for_pred']=='equal':\n",
    "                towards_a=0\n",
    "                towards_b=0\n",
    "                for k in adjacent_segs[i]:\n",
    "                    if k in ranked_a:\n",
    "                        towards_a=towards_a+1\n",
    "                    elif k in ranked_b:\n",
    "                        towards_b=towards_b+1\n",
    "                if towards_a>=towards_b:\n",
    "                    for kk in range(len(ranked_a_list)):\n",
    "                        if ranked_a_list[kk]['mask_id']==i:\n",
    "                            ranked_a_list[kk]['required_for_pred']=True\n",
    "                            ranked_a_list[kk]['once_equals']=True\n",
    "                    for kk in range(len(ranked_b_list)):\n",
    "                        if ranked_b_list[kk]['mask_id']==i:\n",
    "                            ranked_b_list[kk]['required_for_pred']=False\n",
    "                            ranked_b_list[kk]['once_equals']=True\n",
    "                else:\n",
    "                    for kk in range(len(ranked_a_list)):\n",
    "                        if ranked_a_list[kk]['mask_id']==i:\n",
    "                            ranked_a_list[kk]['required_for_pred']=False\n",
    "                            ranked_a_list[kk]['once_equals']=True\n",
    "                    for kk in range(len(ranked_b_list)):\n",
    "                        if ranked_b_list[kk]['mask_id']==i:\n",
    "                            ranked_b_list[kk]['required_for_pred']=True\n",
    "                            ranked_b_list[kk]['once_equals']=True\n",
    "                        \n",
    "                    \n",
    "    # print(adjacent_segs)\n",
    "    # print('********************************')        \n",
    "    # print(ranked_a_list)\n",
    "    # print('********************************')\n",
    "    # print(ranked_b_list)\n",
    "    return adjacent_segs,ranked_a_list,ranked_b_list\n",
    "\n",
    "\n",
    "def get_adjacent_segments(new_segs,x):\n",
    "    shape=new_segs[x].shape\n",
    "    x1=new_segs[x]\n",
    "    new_array=copy.deepcopy(x1)\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[0]):\n",
    "            if x1[i][j]==True:\n",
    "                if i+1 < shape[0] and x1[i+1][j]==False:\n",
    "                    new_array[i+1][j]=True\n",
    "                if i-1 >= 0 and x1[i-1][j]==False:\n",
    "                    new_array[i-1][j]=True\n",
    "                if j+1 < shape[0] and x1[i][j+1]==False:\n",
    "                    new_array[i][j+1]=True\n",
    "                if j-1 >= 0 and x1[i][j-1]==False:\n",
    "                    new_array[i][j-1]=True\n",
    "    x3=[]\n",
    "    for i in range(len(new_segs)):\n",
    "        x2=np.logical_and(new_segs[i],new_array)\n",
    "        if np.any(x2)==True and i!=x:\n",
    "            x3.append(i)\n",
    "    return x3\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "758dfb19-a1b8-4613-a242-7b3fa61a0e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for disjoint experiment\n",
    "\n",
    "\n",
    "def calculate_predictions_for_each_redaction_new(model_in_use,segmentwise_attributions,image,im,topk,top10list,new_segs,prediction1,prediction2,ranked_list,predictions):\n",
    "    numbered_masks = {ind: mask for ind, mask in enumerate(new_segs)}\n",
    "    masking_type='normal/'\n",
    "    dict_for_redacted_images=['temp']\n",
    "    dict_for_redacted_masks=['temp']\n",
    "    dict1={}\n",
    "    dict1[prediction1]=[]\n",
    "    dict1[prediction2]=[]\n",
    "    dict1[prediction1].append(predictions[0][prediction1].numpy())\n",
    "    dict1[prediction2].append(predictions[0][prediction2].numpy())\n",
    "    dict2={}\n",
    "    dict2[prediction1]=[]\n",
    "    dict2[prediction2]=[]\n",
    "    oldmask=np.zeros(numbered_masks[0].shape, dtype=bool)\n",
    "    redact_segments_for_this=[True,False]\n",
    "    \n",
    "    for k in redact_segments_for_this:\n",
    "        for i in ranked_list:\n",
    "            if k == i['required_for_pred']:\n",
    "                mask=numbered_masks[i['mask_id']]\n",
    "                mask=np.logical_or(mask,oldmask)\n",
    "                new_im=mask_on_image(im , mask, masking_type)\n",
    "                new_im1=mask_on_image(image , mask, masking_type)\n",
    "                oldmask=mask\n",
    "                (predictions1,decoded_predictions1,top10list1,top1000list1,id_to_name1)=predict_img(model_in_use,new_im,print_data=False)\n",
    "                \n",
    "                # dict_for_redacted_images.append(new_im1) \n",
    "                # dict_for_redacted_masks.append(mask) \n",
    "                \n",
    "                dict1[prediction1].append(predictions1[0][prediction1].numpy())\n",
    "                dict1[prediction2].append(predictions1[0][prediction2].numpy())\n",
    "\n",
    "        if k == True:\n",
    "            done_with_one_set=len(dict1[prediction1])\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    # print(dict_for_redacted_images)\n",
    "    for key in dict1:\n",
    "        for k in range(len(dict1[key])):\n",
    "            dict2[key].append((dict1[key][k]/dict1[key][0])*100)\n",
    "    return dict1,dict2,dict_for_redacted_images,dict_for_redacted_masks,done_with_one_set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "accc2781-5ea5-44e8-9bc4-7f547fe534f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for disjoint experiment\n",
    "def compare_topk(model_in_use,segmentwise_attributions,image,im,topk,top10list,new_segs,predictions,pth,list_of_masks,redo=False):\n",
    "    if os.path.exists(pth+'_pred_dict'+'.pkl') and redo==False:\n",
    "        with open(pth+'_pred_dict'+'.pkl', 'rb') as file:\n",
    "            prediction_dictionary=pickle.load(file)\n",
    "        print('loaded final predictions after redactions from file')\n",
    "        return prediction_dictionary\n",
    "    \n",
    "    else:\n",
    "        print('starting redactions')\n",
    "        combs=[i for i in range(topk)]\n",
    "        cnt=0\n",
    "        prediction_dictionary={}\n",
    "        for (x1,x2) in list(combinations(combs, 2)):\n",
    "            prediction1=top10list[x1]\n",
    "            prediction2=top10list[x2]\n",
    "            prediction_dictionary[f'{prediction1} vs {prediction2}']={}\n",
    "            prediction_dictionary[f'{prediction1} vs {prediction2}'][f'{prediction1}']=[]\n",
    "            prediction_dictionary[f'{prediction1} vs {prediction2}'][f'{prediction2}']=[]\n",
    "            adjacent_segs,ranked_pred1_list,ranked_pred2_list = rank_the_disjoint_segments_based_on_predictions(list_of_masks,prediction1,prediction2,new_segs)\n",
    "            prediction_dictionary1,prediction_dictionary_with_percentages1,dict_for_redacted_images1,dict_for_redacted_masks1,done_with_one_set1=calculate_predictions_for_each_redaction_new(model_in_use,segmentwise_attributions,image,im,topk,top10list,new_segs,prediction1,prediction2,ranked_pred1_list,predictions)\n",
    "            prediction_dictionary2,prediction_dictionary_with_percentages2,dict_for_redacted_images2,dict_for_redacted_masks2,done_with_one_set2=calculate_predictions_for_each_redaction_new(model_in_use,segmentwise_attributions,image,im,topk,top10list,new_segs,prediction2,prediction1,ranked_pred2_list,predictions)\n",
    "            # print(dict_for_redacted_images1)\n",
    "            prediction_dictionary[f'{prediction1} vs {prediction2}'][f'{prediction1}'].append(prediction_dictionary1)\n",
    "            prediction_dictionary[f'{prediction1} vs {prediction2}'][f'{prediction1}'].append(prediction_dictionary_with_percentages1)\n",
    "            prediction_dictionary[f'{prediction1} vs {prediction2}'][f'{prediction1}'].append(dict_for_redacted_masks1)\n",
    "            prediction_dictionary[f'{prediction1} vs {prediction2}'][f'{prediction1}'].append(done_with_one_set1)\n",
    "            prediction_dictionary[f'{prediction1} vs {prediction2}'][f'{prediction1}'].append(ranked_pred1_list)\n",
    "            \n",
    "            prediction_dictionary[f'{prediction1} vs {prediction2}'][f'{prediction2}'].append(prediction_dictionary2)\n",
    "            prediction_dictionary[f'{prediction1} vs {prediction2}'][f'{prediction2}'].append(prediction_dictionary_with_percentages2)\n",
    "            prediction_dictionary[f'{prediction1} vs {prediction2}'][f'{prediction2}'].append(dict_for_redacted_masks2)\n",
    "            prediction_dictionary[f'{prediction1} vs {prediction2}'][f'{prediction2}'].append(done_with_one_set2)\n",
    "            prediction_dictionary[f'{prediction1} vs {prediction2}'][f'{prediction2}'].append(ranked_pred2_list)\n",
    "\n",
    "            prediction_dictionary[f'{prediction1} vs {prediction2}']['adjacent_segs']=adjacent_segs\n",
    "        print('completed redactions')\n",
    "        with open(pth+'_pred_dict'+'.pkl', 'wb') as file:\n",
    "            pickle.dump(prediction_dictionary, file)\n",
    "            file.close()\n",
    "        return prediction_dictionary\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "735bc345-62cc-49ae-a000-6d2997aea25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for disjoint experiment\n",
    "\n",
    "def plot_for_type1_redaction1_new(redacted_image_with_pred1,redacted_mask_with_pred1,first_redaction,id_to_name,prediction1,prediction2,redaction1_percentile,first_redaction_all):\n",
    "    ROWS=1\n",
    "    COLS=2\n",
    "    UPSCALE_FACTOR = 10\n",
    "    fig, axs = plt.subplots(ROWS, COLS, figsize=(ROWS * UPSCALE_FACTOR, COLS * (UPSCALE_FACTOR-5)))\n",
    "    axs.flat[0].imshow(redacted_image_with_pred1)\n",
    "    axs.flat[1].imshow(redacted_mask_with_pred1)\n",
    "    \n",
    "    axs.flat[0].axis('off')\n",
    "    axs.flat[1].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    ROWS=1\n",
    "    COLS=2\n",
    "    UPSCALE_FACTOR = 10\n",
    "    fig, axs = plt.subplots(ROWS, COLS, figsize=(ROWS * UPSCALE_FACTOR, COLS * (UPSCALE_FACTOR-8)))\n",
    "    axs.flat[0].plot(range(len(first_redaction_all[0][prediction1])),first_redaction_all[0][prediction1],label=id_to_name[prediction1])\n",
    "    axs.flat[0].plot(range(len(first_redaction_all[0][prediction2])),first_redaction_all[0][prediction2],label=id_to_name[prediction2])\n",
    "    axs.flat[0].legend()\n",
    "    axs.flat[0].set_title('probabilities vs no. of redacted segments')\n",
    "    axs.flat[0].set_ylabel('probabilities')\n",
    "    axs.flat[0].set_xlabel('no. of redacted segments')\n",
    "    axs.flat[0].axvline(x=redaction1_percentile, color='r', linestyle=':',label=str(redaction1_percentile))\n",
    "    axs.flat[0].axvline(x=first_redaction_all[3], color='b', linestyle=':',label=str(first_redaction_all[3]))\n",
    "    axs.flat[1].plot(range(len(first_redaction[prediction1])),first_redaction[prediction1],label=id_to_name[prediction1])\n",
    "    axs.flat[1].plot(range(len(first_redaction[prediction2])),first_redaction[prediction2],label=id_to_name[prediction2])\n",
    "    axs.flat[1].legend()\n",
    "    axs.flat[1].set_title('percentages vs no. of redacted segments')\n",
    "    axs.flat[1].set_ylabel('percentages')\n",
    "    axs.flat[1].set_xlabel('no. of redacted segments')\n",
    "    axs.flat[1].axvline(x=redaction1_percentile, color='r', linestyle=':',label=str(redaction1_percentile))\n",
    "    axs.flat[1].axvline(x=first_redaction_all[3], color='b', linestyle=':',label=str(first_redaction_all[3]))\n",
    "    plt.show()\n",
    "\n",
    "def plot_for_type1_redaction2_new(redacted_image_with_pred2,redacted_mask_with_pred2,second_redaction,id_to_name,prediction1,prediction2,redaction2_percentile,second_redaction_all):\n",
    "\n",
    "\n",
    "    ROWS=1\n",
    "    COLS=2\n",
    "    UPSCALE_FACTOR = 10\n",
    "    fig, axs = plt.subplots(ROWS, COLS, figsize=(ROWS * UPSCALE_FACTOR, COLS * (UPSCALE_FACTOR-5)))\n",
    "    axs.flat[0].imshow(redacted_image_with_pred2)\n",
    "    axs.flat[1].imshow(redacted_mask_with_pred2)\n",
    "    axs.flat[0].axis('off')\n",
    "    axs.flat[1].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    ROWS=1\n",
    "    COLS=2\n",
    "    UPSCALE_FACTOR = 10\n",
    "    fig, axs = plt.subplots(ROWS, COLS, figsize=(ROWS * UPSCALE_FACTOR, COLS * (UPSCALE_FACTOR-8)))\n",
    "    axs.flat[0].plot(range(len(second_redaction_all[0][prediction1])),second_redaction_all[0][prediction1],label=id_to_name[prediction1])\n",
    "    axs.flat[0].plot(range(len(second_redaction_all[0][prediction2])),second_redaction_all[0][prediction2],label=id_to_name[prediction2])\n",
    "    axs.flat[0].legend()\n",
    "    axs.flat[0].set_title('probabilities vs no. of redacted segments')\n",
    "    axs.flat[0].set_ylabel('probabilities')\n",
    "    axs.flat[0].set_xlabel('no. of redacted segments')\n",
    "    axs.flat[0].axvline(x=redaction2_percentile, color='r', linestyle=':',label=str(redaction2_percentile))\n",
    "    axs.flat[0].axvline(x=second_redaction_all[3], color='b', linestyle=':',label=str(second_redaction_all[3]))\n",
    "    axs.flat[1].plot(range(len(second_redaction[prediction1])),second_redaction[prediction1],label=id_to_name[prediction1])\n",
    "    axs.flat[1].plot(range(len(second_redaction[prediction2])),second_redaction[prediction2],label=id_to_name[prediction2])\n",
    "    axs.flat[1].legend()\n",
    "    axs.flat[1].set_title('percentages vs no. of redacted segments')\n",
    "    axs.flat[1].set_ylabel('percentages')\n",
    "    axs.flat[1].set_xlabel('no. of redacted segments')\n",
    "    axs.flat[1].axvline(x=redaction2_percentile, color='r', linestyle=':',label=str(redaction2_percentile))\n",
    "    axs.flat[1].axvline(x=second_redaction_all[3], color='b', linestyle=':',label=str(second_redaction_all[3]))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_for_type2_sequence_of_masks(segmentwise_attributions,prediction1,prediction2,redaction1_percentile,redaction2_percentile):\n",
    "    xy=segmentwise_attributions[prediction1] > np.percentile(segmentwise_attributions[prediction1], 100)\n",
    "    mask_temp=np.zeros(xy.shape)\n",
    "    mask_temp_array=[]\n",
    "    mask_temp_array2=[]\n",
    "    for o in range(redaction1_percentile+1):\n",
    "        mask_temp_new = segmentwise_attributions[prediction1] > np.percentile(segmentwise_attributions[prediction1], 100-o)\n",
    "        if np.all(mask_temp_new== mask_temp):\n",
    "            continue\n",
    "        else:\n",
    "            mask_temp= mask_temp_new\n",
    "            mask_temp_array.append(mask_temp_new)\n",
    "            mask_temp_array2.append(o)\n",
    "   \n",
    "    n=len(mask_temp_array)                \n",
    "    # n=redaction1_percentile+1\n",
    "    ROWS = math.floor(math.sqrt(n))\n",
    "    COLS = math.ceil(n / ROWS)\n",
    "    UPSCALE_FACTOR = 10\n",
    "    fig, axs = plt.subplots(ROWS, COLS, figsize=(ROWS * UPSCALE_FACTOR, COLS * (UPSCALE_FACTOR-5)))\n",
    "    for o in range(len(mask_temp_array)):\n",
    "        axs.flat[o].imshow(mask_temp_array[o])\n",
    "        axs.flat[o].set_title(mask_temp_array2[o])\n",
    "    # for o in range(redaction1_percentile+1):\n",
    "    #     mask_temp = segmentwise_attributions[prediction1] > np.percentile(segmentwise_attributions[prediction1], 100-o)\n",
    "    #     axs.flat[o].imshow(mask_temp)\n",
    "    plt.show()\n",
    "    \n",
    "    xy=segmentwise_attributions[prediction1] > np.percentile(segmentwise_attributions[prediction1], 100)\n",
    "    mask_temp=np.zeros(xy.shape)\n",
    "    mask_temp_array=[]\n",
    "    mask_temp_array2=[]\n",
    "    for o in range(redaction2_percentile+1):\n",
    "        mask_temp_new = segmentwise_attributions[prediction2] > np.percentile(segmentwise_attributions[prediction2], 100-o)\n",
    "        if np.all(mask_temp_new== mask_temp):\n",
    "            continue\n",
    "        else:\n",
    "            mask_temp= mask_temp_new\n",
    "            mask_temp_array.append(mask_temp_new)\n",
    "            mask_temp_array2.append(o)\n",
    "    \n",
    "    \n",
    "    n=len(mask_temp_array)                \n",
    "    # n=redaction1_percentile+1\n",
    "    ROWS = math.floor(math.sqrt(n))\n",
    "    COLS = math.ceil(n / ROWS)\n",
    "    UPSCALE_FACTOR = 10\n",
    "    fig, axs = plt.subplots(ROWS, COLS, figsize=(ROWS * UPSCALE_FACTOR, COLS * (UPSCALE_FACTOR-5)))\n",
    "    for o in range(len(mask_temp_array)):\n",
    "        axs.flat[o].imshow(mask_temp_array[o])\n",
    "        axs.flat[o].set_title(mask_temp_array2[o])\n",
    "    # for o in range(redaction1_percentile+1):\n",
    "    #     mask_temp = segmentwise_attributions[prediction1] > np.percentile(segmentwise_attributions[prediction1], 100-o)\n",
    "    #     axs.flat[o].imshow(mask_temp)\n",
    "    plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f38db028-737e-4457-804a-cc75c69f9557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for disjoint experiment\n",
    "\n",
    "\n",
    "\n",
    "def check_for_type_1_and_2_using_rankings_new(segs,pixelwise_attributions,segmentwise_attributions,prediction_dictionary,model_in_use,im,image,topk,top10list,delta_for_type1,checkonly=None):\n",
    "    combs=[i for i in range(2)]\n",
    "    cnt=0\n",
    "    for (x1,x2) in list(combinations(combs, 2)):\n",
    "        prediction1=top10list[x1]\n",
    "        prediction2=top10list[x2]\n",
    "        first_redaction=prediction_dictionary[f'{prediction1} vs {prediction2}'][f'{prediction1}'][1]\n",
    "        second_redaction=prediction_dictionary[f'{prediction1} vs {prediction2}'][f'{prediction2}'][1]\n",
    "\n",
    "        first_redaction_all=prediction_dictionary[f'{prediction1} vs {prediction2}'][f'{prediction1}']\n",
    "        second_redaction_all=prediction_dictionary[f'{prediction1} vs {prediction2}'][f'{prediction2}']\n",
    "        \n",
    "        print(x1,x2)\n",
    "        print(id_to_name[prediction1],id_to_name[prediction2])\n",
    "        print(prediction1,prediction2)\n",
    "        type1_flag=None\n",
    "        type2_flag=None\n",
    "\n",
    "        print('adjacent segs')\n",
    "        print(prediction_dictionary[f'{prediction1} vs {prediction2}']['adjacent_segs'])\n",
    "        \n",
    "        if checkonly=='type1' or checkonly==None:\n",
    "            # checking for type1\n",
    "            print('checking for type1')\n",
    "            for i in range(len(first_redaction[prediction1])):\n",
    "                flag1=None\n",
    "                if first_redaction[prediction1][i] <= 100-delta_for_type1 and first_redaction[prediction2][i] >= delta_for_type1:\n",
    "                    flag1=True\n",
    "                    redaction1_percentile=i\n",
    "                    break\n",
    "        \n",
    "            for i in range(len(first_redaction[prediction1])):\n",
    "                flag2=None    \n",
    "                if second_redaction[prediction2][i] <= 100-delta_for_type1 and second_redaction[prediction1][i] >= delta_for_type1:\n",
    "                    flag2=True\n",
    "                    redaction2_percentile=i\n",
    "                    break\n",
    "        \n",
    "            if flag1==True and flag2==True:\n",
    "                type1_flag=True\n",
    "                cnt=cnt+1\n",
    "                print('Yes its type1')\n",
    "\n",
    "                \n",
    "                masking_type='normal/'\n",
    "                \n",
    "                print('redaction for '+str(prediction1)+' percentile at ',redaction1_percentile)\n",
    "                print(' Initial predcition of '+str(prediction1),first_redaction_all[0][prediction1][0])\n",
    "                print(' final predcition of '+str(prediction1),first_redaction_all[0][prediction1][redaction1_percentile])\n",
    "                print(' final percentage of '+str(prediction1),first_redaction[prediction1][redaction1_percentile])\n",
    "                print(' Initial predcition of '+str(prediction2),first_redaction_all[0][prediction2][0])\n",
    "                print(' final predcition of '+str(prediction2),first_redaction_all[0][prediction2][redaction1_percentile])\n",
    "                print(' final percentage of '+str(prediction2),first_redaction[prediction2][redaction1_percentile])\n",
    "    \n",
    "    \n",
    "                redacted_mask_with_pred1=first_redaction_all[2][redaction1_percentile]\n",
    "                redacted_image_with_pred1=mask_on_image(image , redacted_mask_with_pred1, masking_type)\n",
    "                plot_for_type1_redaction1_new(redacted_image_with_pred1,redacted_mask_with_pred1,first_redaction,id_to_name,prediction1,prediction2,redaction1_percentile,first_redaction_all)\n",
    "    \n",
    "                # print(len(first_redaction[prediction1]))\n",
    "                \n",
    "                print('redaction for '+str(prediction2)+' percentile at ',redaction2_percentile)\n",
    "                print(' Initial predcition of '+str(prediction2),second_redaction_all[0][prediction2][0])\n",
    "                print(' final predcition of '+str(prediction2),second_redaction_all[0][prediction2][redaction2_percentile])\n",
    "                print(' final percentage of '+str(prediction2),second_redaction[prediction2][redaction2_percentile])\n",
    "                print(' Initial predcition of '+str(prediction1),second_redaction_all[0][prediction1][0])\n",
    "                print(' final predcition of '+str(prediction1),second_redaction_all[0][prediction1][redaction2_percentile])\n",
    "                print(' final percentage of '+str(prediction1),second_redaction[prediction1][redaction2_percentile])\n",
    "    \n",
    "\n",
    "                redacted_mask_with_pred2=second_redaction_all[2][redaction2_percentile]\n",
    "                redacted_image_with_pred2=mask_on_image(image , redacted_mask_with_pred2, masking_type)\n",
    "                plot_for_type1_redaction2_new(redacted_image_with_pred2,redacted_mask_with_pred2,second_redaction,id_to_name,prediction1,prediction2,redaction2_percentile,second_redaction_all)\n",
    "    \n",
    "                \n",
    "                # mask_for_redaction1 = segmentwise_attributions[prediction1] > np.percentile(segmentwise_attributions[prediction1], 100-redaction1_percentile)\n",
    "                # mask_for_redaction2 = segmentwise_attributions[prediction2] > np.percentile(segmentwise_attributions[prediction2], 100-redaction2_percentile)\n",
    "        \n",
    "                \n",
    "                \n",
    "                # print(id_to_name[prediction1])\n",
    "                # plt.imshow(redacted_image_with_pred1)\n",
    "                # plt.show()\n",
    "                # print(id_to_name[prediction2])\n",
    "                # plt.imshow(redacted_image_with_pred2)\n",
    "                # plt.show()\n",
    "\n",
    "\n",
    "        '''if checkonly=='type2' or checkonly==None and type1_flag!=True:\n",
    "            #checking for type2\n",
    "            print('checking for type2')\n",
    "            # print(first_redaction[prediction1])\n",
    "            # print(first_redaction[prediction2])\n",
    "            for i in range(len(first_redaction[prediction1])):\n",
    "                flag1=None\n",
    "                # print(i,first_redaction[prediction1][i],first_redaction[prediction2][i])\n",
    "                if first_redaction[prediction1][i] <= 20 and first_redaction[prediction2][i] <= 20:\n",
    "                    flag1=True\n",
    "                    redaction1_percentile=i\n",
    "                    break\n",
    "        \n",
    "            for i in range(len(first_redaction[prediction1])):\n",
    "                flag2=None\n",
    "                # print(i,second_redaction[prediction2][i],second_redaction[prediction1][i])\n",
    "        \n",
    "                if second_redaction[prediction2][i] <= 20 and second_redaction[prediction1][i] <= 20:\n",
    "                    flag2=True\n",
    "                    redaction2_percentile=i\n",
    "                    break\n",
    "            # print(flag1,flag2)\n",
    "            if flag1==True and flag2==True:\n",
    "                \n",
    "                # mask_for_redaction1 = segmentwise_attributions[prediction1] > np.percentile(segmentwise_attributions[prediction1], 100-redaction1_percentile)\n",
    "                # mask_for_redaction2 = segmentwise_attributions[prediction2] > np.percentile(segmentwise_attributions[prediction2], 100-redaction2_percentile)\n",
    "    \n",
    "                # plt.imshow(mask_for_redaction1)\n",
    "                # plt.show()\n",
    "                # plt.imshow(new_im1)\n",
    "                # plt.show()\n",
    "                # plt.imshow(mask_for_redaction2)\n",
    "                # plt.show()\n",
    "                # plt.imshow(new_im1)\n",
    "                # plt.show()\n",
    "                # mask_union=np.logical_or(mask_for_redaction1, mask_for_redaction2) \n",
    "                # mask_intersection=np.logical_and(mask_for_redaction1, mask_for_redaction2) \n",
    "                # mask_only_for_1=(mask_union.astype(float)-mask_for_redaction2.astype(float)).astype(bool)\n",
    "                # mask_only_for_2=(mask_union.astype(float)-mask_for_redaction1.astype(float)).astype(bool)\n",
    "                \n",
    "                \n",
    "        \n",
    "                # redacted_image_with_pred1,redacted_mask_with_pred1=show_redacted_image(segmentwise_attributions,prediction1,redaction1_percentile)\n",
    "                # redacted_image_with_pred2,redacted_mask_with_pred2=show_redacted_image(segmentwise_attributions,prediction2,redaction2_percentile)\n",
    "                \n",
    "                \n",
    "                # masking_type='normal/'\n",
    "                # new_im=mask_on_image(im , mask_only_for_1, masking_type)\n",
    "                # new_im1=mask_on_image(image , mask_only_for_1, masking_type)\n",
    "                # (predictions1,decoded_predictions1,top10list1,top1000list1,id_to_name1)=predict_img(model_in_use,new_im,print_data=False)\n",
    "                # new_prediction1_percentage_after_only1=(predictions1[0][prediction1].numpy()/final_prediction_dictionary[prediction1][prediction1][0])*100\n",
    "                # new_prediction2_percentage_after_only1=(predictions1[0][prediction2].numpy()/final_prediction_dictionary[prediction2][prediction2][0])*100\n",
    "                \n",
    "                # # plt.imshow(mask_only_for_1)\n",
    "                # # plt.show()\n",
    "                # # plt.imshow(new_im1)\n",
    "                # # plt.show()\n",
    "                # masking_type='normal/'\n",
    "                # new_im=mask_on_image(im , mask_only_for_2, masking_type)\n",
    "                # new_im1=mask_on_image(image , mask_only_for_2, masking_type)\n",
    "                # (predictions2,decoded_predictions2,top10list2,top1000list2,id_to_name2)=predict_img(model_in_use,new_im,print_data=False)\n",
    "                # new_prediction1_percentage_after_only2=(predictions2[0][prediction1].numpy()/final_prediction_dictionary[prediction1][prediction1][0])*100\n",
    "                # new_prediction2_percentage_after_only2=(predictions2[0][prediction2].numpy()/final_prediction_dictionary[prediction2][prediction2][0])*100\n",
    "                # # plt.imshow(mask_only_for_2)\n",
    "                # # plt.show()\n",
    "                \n",
    "                # masking_type='normal/'\n",
    "                # new_im=mask_on_image(im , mask_intersection, masking_type)\n",
    "                # new_im1=mask_on_image(image , mask_intersection, masking_type)\n",
    "                # (predictions3,decoded_predictions3,top10list3,top1000list3,id_to_name3)=predict_img(model_in_use,new_im,print_data=False)\n",
    "                # new_prediction1_percentage_after_intersection=(predictions3[0][prediction1].numpy()/final_prediction_dictionary[prediction1][prediction1][0])*100\n",
    "                # new_prediction2_percentage_after_intersection=(predictions3[0][prediction2].numpy()/final_prediction_dictionary[prediction2][prediction2][0])*100\n",
    "                \n",
    "               \n",
    "                \n",
    "                \n",
    "                \n",
    "                masking_type='normal/'\n",
    "                \n",
    "                print('redaction for '+str(prediction1)+' percentile at ',redaction1_percentile)\n",
    "                print(' Initial predcition of '+str(prediction1),first_redaction_all[0][prediction1][0])\n",
    "                print(' final predcition of '+str(prediction1),first_redaction_all[0][prediction1][redaction1_percentile])\n",
    "                print(' final percentage of '+str(prediction1),first_redaction[prediction1][redaction1_percentile])\n",
    "                print(' Initial predcition of '+str(prediction2),first_redaction_all[0][prediction2][0])\n",
    "                print(' final predcition of '+str(prediction2),first_redaction_all[0][prediction2][redaction1_percentile])\n",
    "                print(' final percentage of '+str(prediction2),first_redaction[prediction2][redaction1_percentile])\n",
    "    \n",
    "    \n",
    "                redacted_mask_with_pred1=first_redaction_all[2][redaction1_percentile]\n",
    "                redacted_image_with_pred1=mask_on_image(image , redacted_mask_with_pred1, masking_type)\n",
    "                plot_for_type1_redaction1_new(redacted_image_with_pred1,redacted_mask_with_pred1,first_redaction,id_to_name,prediction1,prediction2,redaction1_percentile,first_redaction_all)\n",
    "    \n",
    "                # print(len(first_redaction[prediction1]))\n",
    "                \n",
    "                print('redaction for '+str(prediction2)+' percentile at ',redaction2_percentile)\n",
    "                print(' Initial predcition of '+str(prediction2),second_redaction_all[0][prediction2][0])\n",
    "                print(' final predcition of '+str(prediction2),second_redaction_all[0][prediction2][redaction2_percentile])\n",
    "                print(' final percentage of '+str(prediction2),second_redaction[prediction2][redaction2_percentile])\n",
    "                print(' Initial predcition of '+str(prediction1),second_redaction_all[0][prediction1][0])\n",
    "                print(' final predcition of '+str(prediction1),second_redaction_all[0][prediction1][redaction2_percentile])\n",
    "                print(' final percentage of '+str(prediction1),second_redaction[prediction1][redaction2_percentile])\n",
    "    \n",
    "\n",
    "                redacted_mask_with_pred2=second_redaction_all[2][redaction2_percentile]\n",
    "                redacted_image_with_pred2=mask_on_image(image , redacted_mask_with_pred2, masking_type)\n",
    "                plot_for_type1_redaction2_new(redacted_image_with_pred2,redacted_mask_with_pred2,second_redaction,id_to_name,prediction1,prediction2,redaction2_percentile,second_redaction_all)\n",
    "    \n",
    "\n",
    "                \n",
    "                \n",
    "                # prediction1=top10list[1]\n",
    "                # prediction2=top10list[2]\n",
    "                # prediction_dictionary[f'{prediction1} vs {prediction2}']['adjacent_segs']\n",
    "                mask_temp_array=prediction_dictionary[f'{prediction1} vs {prediction2}'][f'{prediction1}'][2][1:]\n",
    "                n=len(mask_temp_array)                \n",
    "                # n=redaction1_percentile+1\n",
    "                ROWS = math.floor(math.sqrt(n))\n",
    "                COLS = math.ceil(n / ROWS)\n",
    "                UPSCALE_FACTOR = 10\n",
    "                fig, axs = plt.subplots(ROWS, COLS, figsize=(ROWS * UPSCALE_FACTOR, COLS * (UPSCALE_FACTOR-5)))\n",
    "                for o in range(len(mask_temp_array)):\n",
    "                    axs.flat[o].imshow(mask_temp_array[o])\n",
    "                    # axs.flat[o].set_title(mask_temp_array2[o])\n",
    "                # for o in range(redaction1_percentile+1):\n",
    "                #     mask_temp = segmentwise_attributions[prediction1] > np.percentile(segmentwise_attributions[prediction1], 100-o)\n",
    "                #     axs.flat[o].imshow(mask_temp)\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "\n",
    "                print('******************************************')\n",
    "                # prediction1=top10list[1]\n",
    "                # prediction2=top10list[2]\n",
    "                # prediction_dictionary[f'{prediction1} vs {prediction2}']['adjacent_segs']\n",
    "                temp_array=list_of_masks[prediction1]\n",
    "                # mask_temp_array=prediction_dictionary[f'{prediction1} vs {prediction2}'][f'{prediction1}'][2][1:]\n",
    "                n=len(temp_array)                \n",
    "                # n=redaction1_percentile+1\n",
    "                ROWS = math.floor(math.sqrt(n))\n",
    "                COLS = math.ceil(n / ROWS)\n",
    "                UPSCALE_FACTOR = 10\n",
    "                fig, axs = plt.subplots(ROWS, COLS, figsize=(ROWS * UPSCALE_FACTOR, COLS * (UPSCALE_FACTOR-5)))\n",
    "                for o in range(len(temp_array)):\n",
    "                    axs.flat[o].imshow(new_segs[temp_array[o]])\n",
    "                    axs.flat[o].set_title(f'{temp_array[o]}')\n",
    "                # for o in range(redaction1_percentile+1):\n",
    "                #     mask_temp = segmentwise_attributions[prediction1] > np.percentile(segmentwise_attributions[prediction1], 100-o)\n",
    "                #     axs.flat[o].imshow(mask_temp)\n",
    "                plt.show()\n",
    "                \n",
    "                print('******************************************')\n",
    "                \n",
    "                mask_temp_array=prediction_dictionary[f'{prediction1} vs {prediction2}'][f'{prediction2}'][2][1:]\n",
    "                n=len(mask_temp_array)                \n",
    "                # n=redaction1_percentile+1\n",
    "                ROWS = math.floor(math.sqrt(n))\n",
    "                COLS = math.ceil(n / ROWS)\n",
    "                UPSCALE_FACTOR = 10\n",
    "                fig, axs = plt.subplots(ROWS, COLS, figsize=(ROWS * UPSCALE_FACTOR, COLS * (UPSCALE_FACTOR-5)))\n",
    "                for o in range(len(mask_temp_array)):\n",
    "                    axs.flat[o].imshow(mask_temp_array[o])\n",
    "                    # axs.flat[o].set_title(mask_temp_array2[o])\n",
    "                # for o in range(redaction1_percentile+1):\n",
    "                #     mask_temp = segmentwise_attributions[prediction1] > np.percentile(segmentwise_attributions[prediction1], 100-o)\n",
    "                #     axs.flat[o].imshow(mask_temp)\n",
    "                plt.show()\n",
    "                \n",
    "\n",
    "\n",
    "                print('******************************************')\n",
    "                \n",
    "                # prediction1=top10list[1]\n",
    "                # prediction2=top10list[2]\n",
    "                # prediction_dictionary[f'{prediction1} vs {prediction2}']['adjacent_segs']\n",
    "                temp_array=list_of_masks[prediction2]\n",
    "                # mask_temp_array=prediction_dictionary[f'{prediction1} vs {prediction2}'][f'{prediction1}'][2][1:]\n",
    "                n=len(temp_array)                \n",
    "                # n=redaction1_percentile+1\n",
    "                ROWS = math.floor(math.sqrt(n))\n",
    "                COLS = math.ceil(n / ROWS)\n",
    "                UPSCALE_FACTOR = 10\n",
    "                fig, axs = plt.subplots(ROWS, COLS, figsize=(ROWS * UPSCALE_FACTOR, COLS * (UPSCALE_FACTOR-5)))\n",
    "                for o in range(len(temp_array)):\n",
    "                    axs.flat[o].imshow(new_segs[temp_array[o]])\n",
    "                    axs.flat[o].set_title(f'{temp_array[o]}')\n",
    "                # for o in range(redaction1_percentile+1):\n",
    "                #     mask_temp = segmentwise_attributions[prediction1] > np.percentile(segmentwise_attributions[prediction1], 100-o)\n",
    "                #     axs.flat[o].imshow(mask_temp)\n",
    "                plt.show()    \n",
    "                                    \n",
    "\n",
    "\n",
    "    \n",
    "                # plt.imshow(mask_only_for_2)\n",
    "                # plt.show()\n",
    "                # plt.imshow(new_im1)\n",
    "                # plt.show()\n",
    "                # print(id_to_name[prediction1])\n",
    "                # plt.imshow(redacted_image_with_pred1)\n",
    "                # plt.show()\n",
    "                # plt.imshow(mask_for_redaction1)\n",
    "                # plt.show()\n",
    "                # # plt.imshow(mask_only_for_1)\n",
    "                # # plt.show()\n",
    "                # print(id_to_name[prediction2])\n",
    "                # plt.imshow(redacted_image_with_pred2)\n",
    "                # plt.show()\n",
    "                # plt.imshow(mask_for_redaction2)\n",
    "                # plt.show()\n",
    "                # # plt.imshow(mask_only_for_2)\n",
    "                # # plt.show()\n",
    "                # plt.imshow(mask_union)\n",
    "                # plt.show()\n",
    "                # plt.imshow(mask_intersection)\n",
    "                # plt.show()'''\n",
    "        \n",
    "        break\n",
    "    # if type1_flag==True and type2_flag==True:\n",
    "    #     print('Here i am , Both are true')\n",
    "    # print(cnt)\n",
    "    return cnt\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36fbca8-e04e-408e-938c-2336707bc0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfae656-cfda-42bb-8399-356ba57924d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b3dfa6-6a6f-42fc-9129-bfb3100e843e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
